<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Byoung-gyu Gong</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 01 Mar 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Learning Analytics: Is Underrepresented Students Group Performing Well?</title>
      <link>/post/learning-analytics/learning-analytics-multiple-regression-analysis/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      <guid>/post/learning-analytics/learning-analytics-multiple-regression-analysis/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;learning-analytics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;1. Learning Analytics&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Learning analytics is a computational approach analyzing large-sized students’ learning and education data. It became pretty much popular as the online learning and digitized school administrative platforms produce an infinite stream of students’ data daily. Many higher education institutions actively seek learning analytics to promote data-driven decision-making and evidence-based institutional innovation to enhance students’ performance, retention, and administrative efficiency.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;2. Example Data Analysis&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This post will analyze students’ academic achievement data to see whether the underrepresented student group is performing well enough compared with the other majority group across the time and subject area. The data was artificially and randomly created based on the preconfigured parameter distribution.&lt;/p&gt;
&lt;div id=&#34;explorative-data-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;2.1. Explorative Data Analysis&lt;/strong&gt;&lt;/h3&gt;
&lt;div id=&#34;visual-investigation-histogram&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;2.1.1 Visual Investigation: Histogram&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;UG represents the Underrepresented Group with 1, and non-UG is the other majority students with 0. MATHs means the math subjects with 1 and non-math subjects with 0. In the histogram below, the y-axis means the number of student count, and the x-axis means GPA. The upper histogram is the students’ distribution in the non-math subject area, and the bottom histogram is that in the math subject area. The blue and red dotted line in the middle of the histogram indicates the mean GPA score of UG and non-UG group students. This figure provides us a glimpse of that first; there is a clear gap between UG and non-UG students across all subject areas. However, it is also indicative that the gap has been slightly narrower in the math subjects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### UG vs. Non.UG student performance mean

UG_mean&amp;lt;-Overall_Comp %&amp;gt;% group_by(STDNT_GROUP3,MATHs) %&amp;gt;% 
  summarise(mean=mean(GRD_PTS_PER_UNIT)) 

### Histogram Visualization
  ggplot(Overall_Comp, aes(x=GRD_PTS_PER_UNIT)) +
  geom_histogram(aes(fill=STDNT_GROUP3),position=&amp;quot;dodge&amp;quot;, bins=40, binwidth = 0.3)+
    facet_grid(rows = vars(MATHs), labeller = label_both) +
  labs(title=&amp;quot;GPA Distribution&amp;quot;, x=&amp;quot;GPA&amp;quot;, y=&amp;quot;Count&amp;quot;) +
  theme_minimal() + 
  geom_vline(data=filter(UG_mean, MATHs==0), aes(xintercept=mean),
             color=c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;),linetype=&amp;quot;dashed&amp;quot;,size=0.8) +
  geom_vline(data=filter(UG_mean, MATHs==1), aes(xintercept=mean),
             color=c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;),linetype=&amp;quot;dashed&amp;quot;,size=0.8) +
  scale_fill_manual(name=&amp;quot;UG&amp;quot;,labels = c(&amp;quot;Non-UG&amp;quot;, &amp;quot;UG&amp;quot;),values = c(&amp;quot;#FFCB05&amp;quot;, &amp;quot;#00274C&amp;quot;)) +
    theme(plot.title = element_text(size=14, face=&amp;quot;bold.italic&amp;quot;),
          axis.title.x = element_text(size=14, face=&amp;quot;bold&amp;quot;),
          axis.title.y = element_text(size=14, face=&amp;quot;bold&amp;quot;),
          axis.text.x = element_text(size=10, face=&amp;quot;bold&amp;quot;),
          axis.text.y = element_text(size=10, face=&amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/learning-analytics/2021-03-01-learning-analytics-multiple-regression-analysis.en_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visual-investigation-time-series-change&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;2.1.2 Visual Investigation: Time Series Change&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The graph below shows how UG and non-UG students’ average GPA score changes across the term period. In the non-math courses the gap has been maintained while the gap has been continuously narrowed in the math courses. (Yey! It is excellent news to raise equitable future STEM workforce.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Time series trend analysis
Term_TS_UG&amp;lt;-Overall_Comp %&amp;gt;% 
  select(TERM,Term.Descr,STDNT_GROUP3,GRD_PTS_PER_UNIT, MATHs) %&amp;gt;% 
  group_by(TERM,Term.Descr,STDNT_GROUP3, MATHs) %&amp;gt;% 
  summarise(mean=mean(GRD_PTS_PER_UNIT))

ggplot(Term_TS_UG, aes(x=TERM, y=mean, group=STDNT_GROUP3))+
  scale_x_discrete(labels = unique(Term_TS_UG$Term.Descr))+
  scale_color_manual(name=&amp;quot;UG&amp;quot;,labels = c(&amp;quot;Non-UG&amp;quot;, &amp;quot;UG&amp;quot;),values = c(&amp;quot;#FFCB05&amp;quot;, &amp;quot;#00274C&amp;quot;)) +
  theme_minimal() + 
  geom_smooth(method=loess,aes(color=STDNT_GROUP3), size=1) +
  facet_grid(rows = vars(MATHs), labeller = label_both) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, size=8),
        plot.title = element_text(size=14, face=&amp;quot;bold.italic&amp;quot;),
        axis.title.x = element_text(size=14, face=&amp;quot;bold&amp;quot;),
        axis.title.y = element_text(size=14, face=&amp;quot;bold&amp;quot;),
        axis.text.y = element_text(size=10, face=&amp;quot;bold&amp;quot;)) +
  labs(title=&amp;quot;GPA Change Across Terms&amp;quot;, x=&amp;quot;Term&amp;quot;, y=&amp;quot;GPA Mean&amp;quot;) +
  lims(y=c(2,4))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/learning-analytics/2021-03-01-learning-analytics-multiple-regression-analysis.en_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visual-investigation-interaction&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;2.1.3 Visual Investigation: Interaction&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;I checked the interaction effect between UG membership and taking math classes. If there is a clear interaction, we should spot line-crossing or apparent convergence between the two lines. Here, the result is a bit unclear. We cannot estimate whether there is an interaction or not with this picture.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Interaction plot between CSP membership and Physics class
ggplot(Overall_Comp, aes(x=MATHs, y=GRD_PTS_PER_UNIT, 
                         group=STDNT_GROUP3,color=STDNT_GROUP3)) +
  stat_summary(fun = mean, geom = &amp;quot;point&amp;quot;) +
  stat_summary(fun = mean, geom = &amp;quot;line&amp;quot;) +
  scale_color_manual(name=&amp;quot;UG&amp;quot;,labels = c(&amp;quot;Non-UG&amp;quot;, &amp;quot;UG&amp;quot;),values = c(&amp;quot;darkred&amp;quot;, &amp;quot;#00274C&amp;quot;)) +
  theme_classic2() +
  ylim(y=c(0,4))+
  labs(title=&amp;quot;Interaction between Maths and UG&amp;quot;, x=&amp;quot;Maths&amp;quot;, y=&amp;quot;GPA Mean&amp;quot;) +
    theme(axis.text.x = element_text(size=14, face=&amp;quot;bold&amp;quot;),
          plot.title = element_text(size=14, face=&amp;quot;bold.italic&amp;quot;),
          axis.title.x = element_text(size=14, face=&amp;quot;bold&amp;quot;),
          axis.title.y = element_text(size=14, face=&amp;quot;bold&amp;quot;),
          axis.text.y = element_text(size=14, face=&amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/learning-analytics/2021-03-01-learning-analytics-multiple-regression-analysis.en_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-multiple-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;2.2. Linear Multiple Regression&lt;/strong&gt;&lt;/h3&gt;
&lt;div id=&#34;simplistic-model&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;2.2.1 Simplistic Model&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Then, I ran the regression. First, I made up the simplistic model, just adding UG-membership (STDNT_GROUP3) and math class (MATHs) as independent variables. The result shows that the UG-membership tends to be negatively associated with students’ GPA score and this relationship is statistically significant. It also explains that the students taking math classes tend to score less compared to their counterparts in the other subject classes. It is also statistically significant.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LM2&amp;lt;-lm(GRD_PTS_PER_UNIT ~ STDNT_GROUP3 + MATHs, data=Overall_Comp)
summary(LM2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = GRD_PTS_PER_UNIT ~ STDNT_GROUP3 + MATHs, data = Overall_Comp)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0988 -0.3988  0.2012  0.6012  1.2978 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)    3.0988039  0.0008969 3454.92   &amp;lt;2e-16 ***
## STDNT_GROUP31 -0.3280914  0.0027967 -117.31   &amp;lt;2e-16 ***
## MATHs1        -0.0685440  0.0028988  -23.65   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.9308 on 1296733 degrees of freedom
## Multiple R-squared:  0.01084,    Adjusted R-squared:  0.01084 
## F-statistic:  7106 on 2 and 1296733 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;all-possible-confounders-counted&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;2.2.2 All Possible Confounders Counted&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Then, I added other predictors in our regression model because sometimes the statistical significance in the basic model diminishes due to the other predictors. Although the coefficient score has been slightly decreased, the negative association between GPA and UG-membership and GPA scores and math classes is statistically significant. Also, it indicates male students tend to score lower compared with female students. HSGPA means the high school GPA score, and it is positively associated with their university GPA score. GPAO is the GPA score in the other classes so far in the university. It shows a solid association with the current GPA score.
However, it was revealed no interaction effect between UG-membership and math classes, though it has a negative coefficient. It means that underrepresented group students taking math classes do not under- nor outperform another student group.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LM6&amp;lt;-lm(GRD_PTS_PER_UNIT ~ STDNT_GROUP3 + MATHs + SEX + HSGPA + GPAO +
        STDNT_GROUP3:MATHs, data=Overall_Comp)
summary(LM6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = GRD_PTS_PER_UNIT ~ STDNT_GROUP3 + MATHs + SEX + 
##     HSGPA + GPAO + STDNT_GROUP3:MATHs, data = Overall_Comp)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.9002 -0.3140  0.1327  0.4534  3.9119 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)          -0.2396423  0.0047388 -50.570   &amp;lt;2e-16 ***
## STDNT_GROUP31        -0.1131594  0.0024416 -46.346   &amp;lt;2e-16 ***
## MATHs1               -0.0240013  0.0025247  -9.507   &amp;lt;2e-16 ***
## SEXM                 -0.0206531  0.0013716 -15.058   &amp;lt;2e-16 ***
## HSGPA                 0.0346540  0.0005522  62.761   &amp;lt;2e-16 ***
## GPAO                  1.0003087  0.0013613 734.803   &amp;lt;2e-16 ***
## STDNT_GROUP31:MATHs1 -0.0128971  0.0091333  -1.412    0.158    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.7773 on 1296729 degrees of freedom
## Multiple R-squared:  0.3103, Adjusted R-squared:  0.3103 
## F-statistic: 9.722e+04 on 6 and 1296729 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;multicollinearity-check&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;2.2.3 Multicollinearity Check&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Multicollinearity indicates a degree of association among the independent variables. Once the score exceeds 7 or 9, the model is inefficient because it includes some redundant variables explained by the other independent variables. Fortunately, in this regression model, there is no multicollinearity among the independent variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;car::vif(LM6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       STDNT_GROUP3              MATHs                SEX              HSGPA 
##           1.093532           1.088315           1.008777           1.011272 
##               GPAO STDNT_GROUP3:MATHs 
##           1.030711           1.154940&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficient-change-across-time&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;2.2.4 Coefficient Change Across Time&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;What if we divide the data set according to their terms and establish regression modeling for each term? Would the coefficients be the same across the time? It is just beyond multiple regression, and it may request multilevel modeling, assuming each students’ score observation is embedded in each term, or we can add term as a categorical independent variable. Also, there is a way to include the interaction between the variables and the term.&lt;/p&gt;
&lt;p&gt;However, this may require more complicated data preprocessing and can be felt between multiple regression here. So, I just decided to provide simple visualization, giving us a glimpse of each coefficient’s time-series change. The graphs indicate the coefficient for each term, and the vertical lines indicate confidence intervals. It shows a birds-eye view of the change in coefficients across time.&lt;/p&gt;
&lt;p&gt;Although it is not a rigorous statistical method to figure out coefficient change across time, it provides us some great insight that can build up our next model.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The students’ general GPA continuously been improved across the terms as we can see in the intercept graph.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The underrepresented students’ group has improved their academic performance although their score stays lower than the majority students’ group. The graph “STDNT_GROUP3” shows this upward trend very clearly.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The students dramatically reversed the coefficient value from negative to positive for the math class. This means that in the past, students taking math classes got a lower score comparing with the students in the non-math classes, but these days this tendency was reversed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The high-school GPA (HSGPA) is still a meaningful predictor of the student’s GPA in the university, but its association level is lower than in the past.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There seems to be a seasonal trend in the students’ GPA score, as seen from graphs of Intercept, SEXM, and GPAO.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can see a constant up and down between fall and winter semester. It may indicate that our model has some missing independent variables in explaining the students’ academic performance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Stu_Dist&amp;lt;-Overall_Comp %&amp;gt;% 
  select(TERM, Term.Descr, MATHs, STDNT_GROUP3) %&amp;gt;%
  group_by(TERM,Term.Descr, MATHs, STDNT_GROUP3) %&amp;gt;%
  summarise(n=n()) %&amp;gt;% ungroup() %&amp;gt;%
  group_by(TERM,Term.Descr,MATHs) %&amp;gt;% 
  mutate(prop=n/sum(n)) %&amp;gt;%
  mutate(MATHs=as.factor(MATHs))

# LM Summary by each term

LM_Summary_TERM&amp;lt;-Overall_Comp %&amp;gt;% 
  group_by(TERM) %&amp;gt;%
  do(tidy(lm(GRD_PTS_PER_UNIT ~ STDNT_GROUP3 + MATHs + SEX + HSGPA + GPAO + STDNT_GROUP3:MATHs,
             data=.),conf.int = TRUE))

plot_list=list()
for(i in unique(LM_Summary_TERM$term)){
p&amp;lt;-ggplot(filter(LM_Summary_TERM,term==i), 
       aes(x=TERM, y=estimate)) + 
  scale_x_discrete(labels = unique(Stu_Dist$Term.Descr)) +
  geom_line() +
  geom_point() +
  geom_errorbar(aes(ymax = conf.high, ymin=conf.low), width=0.2) +
  theme_bw() +
  facet_grid(rows=&amp;quot;term&amp;quot;) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) +
  geom_vline(aes(xintercept=0),
             color=c(&amp;quot;red&amp;quot;),linetype=&amp;quot;dashed&amp;quot;,size=0.8)
  plot_list[[i]]=p
}

ggarrange(plot_list[[1]],plot_list[[2]],plot_list[[3]],plot_list[[4]],
          plot_list[[5]],plot_list[[6]],plot_list[[7]],
          ncol = 1, nrow = 7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/learning-analytics/2021-03-01-learning-analytics-multiple-regression-analysis.en_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;3. Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In this post, I examined the academic performance of the underrepresented students group comparing with another non-underrepresented students group in all courses and also in the math courses. My analysis discovered that the underrepresented students group tends to score lower both in all classes and math classes, but the performance gap has been narrowed down across the time. Also, the analysis found that the underrepresented students group does not specifically under-perform in the math classes comparing with the other classes.&lt;/p&gt;
&lt;p&gt;This information can be crucial to make an institutional level policy decision. Fortunately, the result showed that underrepresented students group is improving their academic performance a lot, and even the influence of high school GPA score has been decrease, which means the students’ learning experience in the university contributed to their GPA score. However, it should be noticed that we may need more rigorous test to prove that the students’ performance has been much improved and the gap has been narrowed down across the time. I will address this problem in another posting.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing Nested Data: Comparing Multiple Approaches</title>
      <link>/post/nested-data/analyzing-nested-data-comparing-multiple-approaches/</link>
      <pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/nested-data/analyzing-nested-data-comparing-multiple-approaches/</guid>
      <description>


&lt;div id=&#34;what-is-nested-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;1. What is Nested Data&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Not all data is created to be independent. We often find that the real world data does not meet the strict statistical presumption that each observation should be independent and identically distributed, so-called iid. In many cases of social science studies, the observations sampled from a population tend to correlate because they belong to the same group, region, and culture. Sample data having such common and correlative trait is called nested data.&lt;/p&gt;
&lt;p&gt;For instance, students in the same school share some traits derived from the school and district features. Students in the same school or district tend to have similar socio-economic status (SES) because students are assigned to the schools based on their residential location. Students in the same school also tend to share the same teachers, school leadership, and school resource. In this account, student samples in the same schools may violate iid principle correlating with each other.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;2. Example Data Analysis&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;It can be costly to ignore the nested data structure. This post will show you a difference between the models considering and not considering the nested data structure.&lt;/p&gt;
&lt;p&gt;I used the international students’ data file for this analysis. The students studied in a foreign country through a one-year study abroad program to raise intercultural understanding. I analyzed to what extent the self-efficacy level predicts or explains the level of students’ intercultural understanding.&lt;/p&gt;
&lt;div id=&#34;explorative-data-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;2.1. Explorative Data Analysis&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;This is the summary of the data set. It tells us there are four variables. Intercultural_Understanding and Self-Efficacy are the continuous variables while the rest of Major and Year are categorical variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Summary of the data
summary(Studyabroad)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Intercultural_Understanding Self_Efficacy      Major       Year   
##  Min.   :1.000               Min.   :3.000   Eng   :20   18-19:31  
##  1st Qu.:3.250               1st Qu.:4.208   Noneng:52   19-20:41  
##  Median :4.333               Median :5.190                         
##  Mean   :4.089               Mean   :4.980                         
##  3rd Qu.:5.000               3rd Qu.:5.762                         
##  Max.   :6.000               Max.   :6.000                         
##  NA&amp;#39;s   :2                   NA&amp;#39;s   :1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get a more detailed sense of the data, let’s check the distribution of the data with visualization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Scatter plot
SA &amp;lt;- ggscatter(Studyabroad, x = &amp;quot;Self_Efficacy&amp;quot;, y = &amp;quot;Intercultural_Understanding&amp;quot;,color = &amp;quot;#00AFBB&amp;quot;, size = 1, alpha = 0.6, add = &amp;quot;reg.line&amp;quot;, add.params = list(color = &amp;quot;blue&amp;quot;, fill = &amp;quot;lightgray&amp;quot;),
   conf.int = TRUE, 
   cor.coef = TRUE ,rug = TRUE)+
border()                                         
# Marginal density plot of x (top panel) and y (right panel)
xplot &amp;lt;- ggdensity(Studyabroad, &amp;quot;Self_Efficacy&amp;quot;, fill = &amp;quot;lightgrey&amp;quot;)
yplot &amp;lt;- ggdensity(Studyabroad, &amp;quot;Intercultural_Understanding&amp;quot;, fill = &amp;quot;lightgrey&amp;quot;)+
rotate()
# Cleaning the plots
SA &amp;lt;- SA + rremove(&amp;quot;legend&amp;quot;)
yplot &amp;lt;- yplot + clean_theme() + rremove(&amp;quot;legend&amp;quot;) 
xplot &amp;lt;- xplot + clean_theme() + rremove(&amp;quot;legend&amp;quot;)

library(cowplot)
plot_grid(xplot, NULL, SA, yplot, ncol = 2, align = &amp;quot;hv&amp;quot;, 
      rel_widths = c(2, 1), rel_heights = c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Nested-data/2020-12-10-analyzing-nested-data-comparing-multiple-approaches.en_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simple-linear-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;2.2. Simple Linear Regression&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The easiest way to analyze this data is simple linear regression. The analysis summary indicates that the students’ self-efficacy is positively associated with the level of intercultural understanding. The coefficient estimate is 0.3914, and the p-value is significant at the 0.05 level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Lm0&amp;lt;-lm(Intercultural_Understanding ~ Self_Efficacy, data =  Studyabroad)
summary(Lm0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Intercultural_Understanding ~ Self_Efficacy, data = Studyabroad)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.9404 -0.9175  0.2527  0.9850  1.8409 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept)     2.1298     0.9314   2.287   0.0254 *
## Self_Efficacy   0.3914     0.1841   2.126   0.0372 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.253 on 67 degrees of freedom
##   (3 observations deleted due to missingness)
## Multiple R-squared:  0.06319,    Adjusted R-squared:  0.04921 
## F-statistic:  4.52 on 1 and 67 DF,  p-value: 0.0372&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;alternative-analytic-methods-for-the-nested-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;2.3. Alternative Analytic Methods for the Nested Data&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;However, the data is stratified with other grouping variables. The data ‘Studyabroad’ has a ‘Major’ variable, indicating that each student belongs to a specific major program. The students’ intercultural understanding explained by self-efficacy may be influenced by their majors.&lt;/p&gt;
&lt;p&gt;We can plot the regression line in the above scatter plot, but I will draw two different lines for each group this time. The scatter plot below shows that students in English and non-English majors have very different patterns of association. In general, the students’ intercultural understanding in English majors is related to their self-efficacy level, while the non-English major students showed no relationship between them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SA&amp;lt;-ggscatter(Studyabroad, x = &amp;quot;Self_Efficacy&amp;quot;, y = &amp;quot;Intercultural_Understanding&amp;quot;, size = 0.3,
          combine = TRUE, color = &amp;quot;Major&amp;quot;, palette = &amp;quot;jco&amp;quot;,
          add = &amp;quot;reg.line&amp;quot;, conf.int = TRUE) +
  stat_cor(aes(color = Major), method = &amp;quot;spearman&amp;quot;)
# Marginal density plot of x (top panel) and y (right panel)
xplot &amp;lt;- ggdensity(Studyabroad, &amp;quot;Self_Efficacy&amp;quot;, fill = &amp;quot;Major&amp;quot;,
                   palette = &amp;quot;jco&amp;quot;)
yplot &amp;lt;- ggdensity(Studyabroad, &amp;quot;Intercultural_Understanding&amp;quot;, fill = &amp;quot;Major&amp;quot;,palette = &amp;quot;jco&amp;quot;)+
rotate()
# Cleaning the plots
SA &amp;lt;- SA + rremove(&amp;quot;legend&amp;quot;)
yplot &amp;lt;- yplot + clean_theme() + rremove(&amp;quot;legend&amp;quot;) 
xplot &amp;lt;- xplot + clean_theme() + rremove(&amp;quot;legend&amp;quot;)

library(cowplot)
plot_grid(xplot, NULL, SA, yplot, ncol = 2, align = &amp;quot;hv&amp;quot;, 
      rel_widths = c(2, 1), rel_heights = c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Nested-data/2020-12-10-analyzing-nested-data-comparing-multiple-approaches.en_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The biggest problem of not considering the nested structure of the data is underestimating each parameter estimation’s standard error. This underestimation is fed into a more frequent rejection of the null hypothesis for the coefficient estimation. We have already seen that the simple linear regression model rejected the null hypothesis above.&lt;/p&gt;
&lt;div id=&#34;solution1.-complex-linear-regression-model-with-grouping-dummy-variables&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;2.3.1. Solution1. Complex Linear Regression Model with Grouping Dummy Variables&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;One of the immediate solutions to handle nested data issue is adding grouping dummy variables in the regression modeling.&lt;/p&gt;
&lt;p&gt;The new regression model below included ‘Major,’ one of the grouping variables, in the modeling. Comparing with the previous simple linear regression, the standard error of Self-efficacy’s coefficient increased from 0.1841 to 0.2565. Moreover, the significant test of the coefficient estimation did not reject the null hypothesis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Lm1&amp;lt;-lm(Intercultural_Understanding ~ Self_Efficacy + Major, data =  Studyabroad)
summary(Lm1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Intercultural_Understanding ~ Self_Efficacy + Major, 
##     data = Studyabroad)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.93139 -0.89425  0.05834  1.09501  1.70774 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept)     2.7438     1.0816   2.537   0.0136 *
## Self_Efficacy   0.1925     0.2565   0.750   0.4557  
## MajorNoneng     0.5228     0.4706   1.111   0.2706  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.251 on 66 degrees of freedom
##   (3 observations deleted due to missingness)
## Multiple R-squared:  0.08039,    Adjusted R-squared:  0.05253 
## F-statistic: 2.885 on 2 and 66 DF,  p-value: 0.06293&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;solution2.-hierarchical-linear-modeling-hlm-or-multilevel-modeling&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;2.3.2. Solution2. Hierarchical Linear Modeling (HLM) or Multilevel Modeling&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Then, at this time, I will use hierarchical linear modeling analysis with the lme4 package to compare it with the previous simple linear regression model.&lt;/p&gt;
&lt;p&gt;First, I will create a null model to calculate the intraclass correlation (ICC). This number indicates the proportion of the outcome variable’s total variation explained by between-group variation. The ICC calculation for this data is about 12%, meaning that the students’ difference in affiliation to majors explains the 12% out of the total variation of the intercultural understanding.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
Lme0&amp;lt;-lmer(Intercultural_Understanding ~ 1+(1|Major), data=Studyabroad)
summary(Lme0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: Intercultural_Understanding ~ 1 + (1 | Major)
##    Data: Studyabroad
## 
## REML criterion at convergence: 231.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.2609 -0.6583  0.1094  0.8609  1.4064 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  Major    (Intercept) 0.2082   0.4563  
##  Residual             1.5430   1.2422  
## Number of obs: 70, groups:  Major, 2
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)   3.9655     0.3606   10.99&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ICC Calculation
library(sjstats)
icc(Lme0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # Intraclass Correlation Coefficient
## 
##      Adjusted ICC: 0.119
##   Conditional ICC: 0.119&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, I added self-efficacy as a predictor. The summary statistics indicate that the self-efficacy coefficient is not significant due to its increased standard error estimation. This tells us that similar to the regression model with the grouping dummy variables, the multilevel modeling also avoids underestimating the standard error lowering probability to reject the null hypothesis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lmerTest)
Lme1&amp;lt;-lmer(Intercultural_Understanding ~ 1 + Self_Efficacy + (1+Self_Efficacy|Major), data=Studyabroad, na.action = na.exclude)
summary(Lme1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML. t-tests use Satterthwaite&amp;#39;s method [
## lmerModLmerTest]
## Formula: 
## Intercultural_Understanding ~ 1 + Self_Efficacy + (1 + Self_Efficacy |  
##     Major)
##    Data: Studyabroad
## 
## REML criterion at convergence: 223.9
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.4245 -0.5919  0.1211  0.8689  1.4253 
## 
## Random effects:
##  Groups   Name          Variance Std.Dev. Corr 
##  Major    (Intercept)   34.365   5.862         
##           Self_Efficacy  1.717   1.310    -1.00
##  Residual                1.422   1.193         
## Number of obs: 69, groups:  Major, 2
## 
## Fixed effects:
##               Estimate Std. Error     df t value Pr(&amp;gt;|t|)
## (Intercept)     0.2977     4.2955 0.6712   0.069    0.960
## Self_Efficacy   0.8874     0.9615 0.6535   0.923    0.582
## 
## Correlation of Fixed Effects:
##             (Intr)
## Self_Effccy -0.999
## convergence code: 0
## boundary (singular) fit: see ?isSingular&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;3. Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This post addressed analyzing nested data, which often violates iid assumption for the inferential statistics. I compared the simple regression model with a more complex regression model, including the group dummy variable and multilevel model. This comparison showed that the simple regression model without considering nested data structure tends to underestimate the coefficient’s standard error, thus more frequently rejecting the null hypothesis in the significant test. Thus, to avoid this underestimation of standard error, it is generally recommended to use a more complex regression model with group dummy variables or multilevel modeling analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why is PISA Difficult to Analyze</title>
      <link>/post/why-pisa-is-difficult-to-analyze/why-is-the-pisa-data-hard-to-analyze/</link>
      <pubDate>Sat, 10 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/why-pisa-is-difficult-to-analyze/why-is-the-pisa-data-hard-to-analyze/</guid>
      <description>


&lt;p&gt;By Byoung-gyu Gong&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/Why-PISA-is-difficult-to-analyze/featured.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div id=&#34;what-is-ilsa-and-pisa&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;1. What is ILSA and PISA&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;International large-scale assessment (ILSA) is conducted regularly by international organizations to measure and compare many different countries’ educational performance and status. The Programme for International Student Assessment (PISA) by OECD is the most well-known ILSA testing member and non-member partner country’s 15-year-old students every two years. It measures students’ reading, math, and science using a computer-based instrument. It also surveys to collect more detailed background information of each student, teacher, and school. The PISA 2006 collected this massive size of data from 398,750 students. Thus, it can be said that the PISA is the most comprehensive large-sized international student assessment data set available now.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-is-it-difficult-to-analyze-pisa-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;2. Why is it difficult to analyze PISA data&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Despite the insightful knowledge we can gain from this large international students’ data set, PISA is not easily accessible to most of the researchers to analyze it. That is not because the data access is limited but because it is complexly structured, requesting sophisticated data pre-processing. Even Jerrim, Lopez-Agudo, Marcenaro-Gutierrez, and Shure (2017) said, “in spite of their acquired relevance, there are few studies which really account for the complex survey and test designs that they present and follow the technical procedures suggested by their developers” (p.1). I will illustrate what caution is needed to analyze the data set and provides R solution in this post using a data set of PISA 2006. &lt;/p&gt;
&lt;div id=&#34;two-staged-sampling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;2.1. Two-staged sampling&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The sampling design of the PISA is the reason why the analysis procedure is so complicated. The PISA does not use random sampling and instead follows two-staged sampling: sample schools and then sample students in the participating schools. It makes the sampling errors of population estimates increase. It conflicts with most computer software designed to assume random sampling for statistical analysis and requests more sophisticated analysis procedures. Once the assumption of random sampling is violated, the student data can depend on each other because they may share common school characteristics.&lt;br /&gt;
 &lt;/p&gt;
&lt;p&gt;To avoid such bias, PISA made some complementary measures.
 &lt;/p&gt;
&lt;p&gt;Once you look at the data table imported from the PISA 2006, you would see some strange variable names such as PV1-5 for each reading, math, and science domains. Also, there are some variables having names like W_FSTR1-80. If you just naively wanted to calculate students’ mean or correlation coefficient with a fixed point score, you would be embarrassed to find these unknown variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Use the following packages for the analysis
library(intsvy)
library(dplyr)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(pisa2006,1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   X CNT SCHOOLID STIDSTD  PV1MATH  PV2MATH  PV3MATH  PV4MATH  PV5MATH  PV1READ
## 1 1 ARG        1       1 305.1799 305.9589 264.6752 320.7587 301.2852 357.0519
##    PV2READ  PV3READ  PV4READ  PV5READ  PV1SCIE  PV2SCIE  PV3SCIE  PV4SCIE
## 1 312.1348 281.6554 316.9474 316.1453 395.3131 411.1652 445.6667 351.4869
##    PV5SCIE  PV1INTR  PV2INTR  PV3INTR  PV4INTR  PV5INTR  PV1SUPP  PV2SUPP
## 1 356.1492 539.9939 480.1177 497.9912 370.1956 506.0343 339.0228 395.3084
##    PV3SUPP  PV4SUPP  PV5SUPP   PV1EPS   PV2EPS   PV3EPS   PV4EPS   PV5EPS
## 1 412.5387 313.7517 322.9412 393.4482 384.1234 354.2843 430.7471 417.6925
##     PV1ISI   PV2ISI   PV3ISI   PV4ISI   PV5ISI   PV1USE  PV2USE   PV3USE
## 1 334.7023 304.8632 323.5126 399.9755 386.9209 401.8404 383.191 385.0559
##    PV4USE  PV5USE W_FSTUWT W_FSTR1  W_FSTR2 W_FSTR3  W_FSTR4 W_FSTR5  W_FSTR6
## 1 436.342 469.911  98.8278 52.1041 139.9112 52.1041 139.9112 52.1041 139.9112
##   W_FSTR7 W_FSTR8 W_FSTR9 W_FSTR10 W_FSTR11 W_FSTR12 W_FSTR13 W_FSTR14 W_FSTR15
## 1 52.1041 52.1041 52.1041  52.1041 139.9112 139.9112  52.1041 139.9112 139.9112
##   W_FSTR16 W_FSTR17 W_FSTR18 W_FSTR19 W_FSTR20 W_FSTR21 W_FSTR22 W_FSTR23
## 1  52.1041  52.1041 139.9112 139.9112 139.9112  52.1041 139.9112  52.1041
##   W_FSTR24 W_FSTR25 W_FSTR26 W_FSTR27 W_FSTR28 W_FSTR29 W_FSTR30 W_FSTR31
## 1 139.9112  52.1041 139.9112  52.1041  52.1041  52.1041  52.1041 139.9112
##   W_FSTR32 W_FSTR33 W_FSTR34 W_FSTR35 W_FSTR36 W_FSTR37 W_FSTR38 W_FSTR39
## 1 139.9112  52.1041 139.9112 139.9112  52.1041  52.1041 139.9112 139.9112
##   W_FSTR40 W_FSTR41 W_FSTR42 W_FSTR43 W_FSTR44 W_FSTR45 W_FSTR46 W_FSTR47
## 1 139.9112  52.1041 139.9112  52.1041 139.9112  52.1041 139.9112  52.1041
##   W_FSTR48 W_FSTR49 W_FSTR50 W_FSTR51 W_FSTR52 W_FSTR53 W_FSTR54 W_FSTR55
## 1  52.1041  52.1041  52.1041 139.9112 139.9112  52.1041 139.9112 139.9112
##   W_FSTR56 W_FSTR57 W_FSTR58 W_FSTR59 W_FSTR60 W_FSTR61 W_FSTR62 W_FSTR63
## 1  52.1041  52.1041 139.9112 139.9112 139.9112  52.1041 139.9112  52.1041
##   W_FSTR64 W_FSTR65 W_FSTR66 W_FSTR67 W_FSTR68 W_FSTR69 W_FSTR70 W_FSTR71
## 1 139.9112  52.1041 139.9112  52.1041  52.1041  52.1041  52.1041 139.9112
##   W_FSTR72 W_FSTR73 W_FSTR74 W_FSTR75 W_FSTR76 W_FSTR77 W_FSTR78 W_FSTR79
## 1 139.9112  52.1041 139.9112 139.9112  52.1041  52.1041 139.9112 139.9112
##   W_FSTR80    ESCS
## 1 139.9112 -2.0048&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, what are these variables in the PISA table?
 &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sampling-weight&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;2.2. Sampling Weight&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;To make each sampled student from each school represent each country’s entire population, we need to give a weight to each student variables. The random sample secures each sampled student’s equal probability, but two-stage sampling needs to adjust the probability with a complicated calculation of the weight. Weight is an inverse of probability to be selected as a sample. For instance, we selected ten students out of 100 students, then the probability to be selected is 0.1, and the weight is 10.
 &lt;/p&gt;
&lt;p&gt;In PISA, each school’s probability to be selected is proportional to its size, which is called PPS sampling method. The schools with larger sizes have a higher probability of selection, but the larger schools have a proportionally less within-school probability of selection. The sum of students’ weight for each country is the total number of the students’ population assumed for the study.
 &lt;/p&gt;
&lt;p&gt;There are two options to apply students’ weight for the PISA analysis, applying final weight(W_FSTUWT), and replicated weight(F_FSTR1-80), but in this post, I do not tackle the replicated weight because it is more related to the estimation of standard error, but not have an impact to the mean, correlation, and regression coefficent estimate.
 &lt;/p&gt;
&lt;p&gt;Let me show you an example here with R code. W_FSTUWT is the final students’ weight. You can see that students in the same school have approximately the same school weights with a little variation. School number 00001 has weights around 98, and 00002 has it at about 89.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Select Argentina students&amp;#39; weight information.
WEIGHT&amp;lt;-pisa2006 %&amp;gt;% select(CNT, SCHOOLID, W_FSTUWT) %&amp;gt;% filter(CNT==&amp;quot;ARG&amp;quot;)
# Create a dataframe that can compare Argentina students in three different schools
A&amp;lt;-WEIGHT %&amp;gt;% filter(SCHOOLID==&amp;quot;1&amp;quot;) %&amp;gt;% slice(1:3)
B&amp;lt;-WEIGHT %&amp;gt;% filter(SCHOOLID==&amp;quot;2&amp;quot;) %&amp;gt;% slice(1:3)
C&amp;lt;-WEIGHT %&amp;gt;% filter(SCHOOLID==&amp;quot;3&amp;quot;) %&amp;gt;% slice(1:3)
rbind(A,B,C)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   CNT SCHOOLID W_FSTUWT
## 1 ARG        1  98.8278
## 2 ARG        1  91.7075
## 3 ARG        1  98.8278
## 4 ARG        2  89.3573
## 5 ARG        2  89.6970
## 6 ARG        2  89.6970
## 7 ARG        3 112.7806
## 8 ARG        3 117.0837
## 9 ARG        3 117.0837&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; 
The summation of the students’ weight is equal to the population total population of 15-years-old students in Argentina. It is 523,047.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;WEIGHT %&amp;gt;% filter(CNT==&amp;quot;ARG&amp;quot;) %&amp;gt;% group_by(CNT) %&amp;gt;% summarize(sum=sum(W_FSTUWT))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   CNT       sum
##   &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 ARG   523048.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Also, we can compare the difference in variable means between raw scores and weighted scores. Comparing with the graph ‘B’ with the graph ‘A,’ we can recognize that the graph ‘B’ is more slightly deviated from the regression line, although the difference is negligible for most countries. Some of the countries in specific variables, the bias to the estimate can be significantly large so we should apply the final student weight for the analysis all the time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create weighted mean score (W_MEAN) and non-weighted means score of PV1SCIE (science score)
# Calculate national mean
PV1SCIE&amp;lt;-pisa2006 %&amp;gt;% select(CNT, PV1SCIE, W_FSTUWT) %&amp;gt;% group_by(CNT) %&amp;gt;%  mutate(W_MEAN=weighted.mean(PV1SCIE,W_FSTUWT, na.rm=TRUE), MEAN=mean(PV1SCIE, na.rm=TRUE)) %&amp;gt;% ungroup() %&amp;gt;% select(CNT, W_MEAN, MEAN) %&amp;gt;% unique()

# Create a scatter plot to compare the discrepancy between 
library(ggplot2)
library(ggpubr)
# Create a scatter plot with raw score means (unweighted mean)
Non_weight&amp;lt;-ggplot(PV1SCIE, aes(x=MEAN, y=MEAN)) + geom_point() + geom_text(data=PV1SCIE, aes(label=CNT), position=position_jitter(width=0.1,height=0.1), size=3) + geom_smooth(method=lm, se=TRUE)+stat_cor(method = &amp;quot;pearson&amp;quot;)
# Create a scatter plot with unweighted mean and weighted mean
Weight_non_weight&amp;lt;-ggplot(PV1SCIE, aes(x=MEAN, y=W_MEAN)) + geom_point() + geom_text(data=PV1SCIE, aes(label=CNT), position=position_jitter(width=0.1,height=0.1), size=3) + geom_smooth(method=lm, se=TRUE)+stat_cor(method = &amp;quot;pearson&amp;quot;)
# Compore these two
ggarrange(Non_weight, Weight_non_weight, 
          labels = c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;),
          ncol = 2, nrow = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;
## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Why-PISA-is-difficult-to-analyze/2020-10-10-why-is-the-pisa-data-hard-to-analyze.en_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plausible-valuepv&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;2.3. Plausible Value(PV)&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;PV is called plausible value, and each domain has five PVs. For instance, math has PV1MATH - PV5MATH, and science has PV1SCIE - PV5SCIE. The PVs are introduced here to adjust measurement error that can be caused by
- the concept to be measured is not clear,
- students physical and psychological condition affected at the moment of testing, and
- the testing environment on a day of testing.
Thus, if we have five different plausible values distributed in a certain range of raw scores, we can avoid point estimate of the students’ competency, which has a high risk of measurement error as mentioned above. It means that we interpret students’ competency as a probability distribution among the plausible values, not as a fixed point. In addition to this rational, the PV was adopted in a practical and efficient standpoint. In PISA tests, students do not take all the test items as it is time-consuming, and each student takes different sets of tests, while the test items missing in each student are treated as a missing value. It creates five different sets of literally plausible score values of the students.
 &lt;/p&gt;
&lt;p&gt;I created two columns of mean data to compare weighted PV means and weighted means of each country. However, as we can see from the below table, there is no significant difference between the weighted PV mean and just the weighted mean with the final weight. The OECD also admitted that PV value does not bring significant impact to reduce bias in the estimate (OECD, 2009)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a dataframe with weighted PV means (weighted by final weight)
PVSCIE&amp;lt;-pisa.mean.pv(pvlabel=&amp;quot;SCIE&amp;quot;, by=&amp;quot;CNT&amp;quot;, data=pisa2006)
# Create a dataframe with weighted means (weighted by final weight)
SCIE&amp;lt;-pisa2006 %&amp;gt;% group_by(CNT) %&amp;gt;% summarise(Mean=(weighted.mean(PV4SCIE,W_FSTUWT, na.rm=TRUE))) %&amp;gt;% mutate(Mean=round(Mean, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Merge&amp;lt;-left_join(PVSCIE, SCIE, by=&amp;quot;CNT&amp;quot;) %&amp;gt;% select(CNT, Freq, Mean.x, Mean.y) %&amp;gt;% rename(PVmean=&amp;quot;Mean.x&amp;quot;, Rawmean=&amp;quot;Mean.y&amp;quot;)
head(Merge,15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    CNT  Freq PVmean Rawmean
## 1  ARG  4339 391.24  391.33
## 2  AUS 14170 526.88  526.90
## 3  AUT  4927 510.84  510.95
## 4  AZE  5184 382.33  381.78
## 5  BEL  8857 510.36  510.62
## 6  BGR  4498 434.08  433.27
## 7  BRA  9295 390.33  389.43
## 8  CAN 22646 534.47  534.28
## 9  CHE 12192 511.52  511.71
## 10 CHL  5233 438.18  438.26
## 11 COL  4478 388.04  388.60
## 12 CZE  5932 512.86  512.69
## 13 DEU  4891 515.65  516.32
## 14 DNK  4532 495.89  495.05
## 15 ESP 19604 488.42  487.51&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;3. Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;As we examined so far, the students’ final weight is a significant factor to reduce bias in estimation. Although PV value has less impact to the estimation and OECD also admit it, but still it should be best to follow the PISA instruction.&lt;/p&gt;
&lt;p&gt;In this post, I only explored the estimates at each country level. However, it is very different for the case of cross-national analysis, setting each country as a unit of analysis. For instance, the weight that should be applied to each student should be adjusted. PISA also provides instruction on how to adjust the weight though there is still controversy over it. I will post this topic later on.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Jerrim, J., Lopez-Agudo, L. A., Marcenaro-Gutierrez, O. D., &amp;amp; Shure, N. (2017, June). To weight or not to weight?: the case of PISA data. In Proceedings of the XXVI Meeting of the Economics of Education Association, Murcia, Spain (pp. 29-30).&lt;/p&gt;
&lt;p&gt;OECD (2009). PISA 2006 Technical Report. OECD Publishing.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Student contact network at school: Spread of the virus</title>
      <link>/post/aspb/network/</link>
      <pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/aspb/network/</guid>
      <description>


&lt;p&gt;By Byoung-gyu Gong&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div id=&#34;pandemic-and-school-reopening-issue&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pandemic and School Reopening Issue&lt;/h2&gt;
&lt;p&gt;With an outbreak of the COVID-19 pandemic, there is increasing attention to the school closer issue. Many countries implemented a strict lock-down to slow down the spread of the virus, but this came at the expense of losing our children’s learning opportunity. Despite that, the reopening school can be very costly as children can transmit the virus to their families and communities. Now that there is significant uncertainty on the impact of school reopening, we see that many countries are still under discussion on this issue. Disappointingly, we do not know much about how the school affects the children’s virus infection.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;purpose-of-the-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Purpose of the Analysis&lt;/h2&gt;
&lt;p&gt;Given this situation, in this post, I will analyze children’s physical contact network at school to see how much contact happens among children. The contact network indicators at school can be a basic knowledge we can reorganize our school space, time, and activity to adjust to the post-pandemic world.&lt;br /&gt;
 &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-source&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Source&lt;/h2&gt;
&lt;p&gt;For the network analysis, I prepared two data sets from the study of &lt;em&gt;Gemnetto, Barrat, &amp;amp; Cattuto (2014) titled Mitigation of infectious disease at school: Targeted class closure vs school closure&lt;/em&gt;, which studied the infection network at the primary school. You can also find the data of this study from &lt;a href=&#34;http://www.sociopatterns.org/datasets/primary-school-temporal-network-data/&#34;&gt;Sociopatterns&lt;/a&gt;. It is the open-source network data platform readily available for any studies and researches. The students’ contact network data provides a fundamental sense to design and implement a plan for contact tracing and social distance at each school level.&lt;/p&gt;
&lt;p&gt;The authors collected this contact network data using a wearable device that records contact whenever students get closer over a certain threshold. The data was collected from an elementary school in France.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analysis&lt;/h2&gt;
&lt;p&gt;For the analysis, I will use igraph package in R program. Using this package, I will calculate centrality scores to detect the central node in the school network and network structure indicators, such as network density and homophily score. Also, I will visualize the network to show you the birds-eye view of the school network.&lt;/p&gt;
&lt;div id=&#34;data-pre-processing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;Data pre-processing&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;First, install the required packages and open the library.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(c(&amp;quot;igraph&amp;quot;,&amp;quot;readr&amp;quot;,&amp;quot;tidyr&amp;quot;,&amp;quot;RColorBrewer&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, you can download the data file directly from my github repository through the following code. (If there is an error message, you should try it multiple times until you can get access to it.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#1. Read data from the Github repository csv files
urlfile1=&amp;quot;https://raw.githubusercontent.com/Arizonagong/vCIES2020_Network-Analysis/master/igraph/primaryschool.csv&amp;quot;
urlfile2=&amp;quot;https://raw.githubusercontent.com/Arizonagong/vCIES2020_Network-Analysis/master/igraph/metadata_primaryschool.csv&amp;quot;
D&amp;lt;-read_csv(url(urlfile1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ── Column specification ─────────────────────────────────────────────────────
## cols(
##   Source = col_double(),
##   Target = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D_meta&amp;lt;-read_csv(url(urlfile2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ── Column specification ─────────────────────────────────────────────────────
## cols(
##   ID = col_double(),
##   Class = col_character(),
##   Gender = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s look into the data set. The first dataset names as “D” is an edge list indicating contact between the students. It is represented like 1234-4424, which means one contact between the student 1234 and 4424.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(D, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 2
##   Source Target
##    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1   1558   1567
## 2   1560   1570
## 3   1567   1574
## 4   1632   1818
## 5   1632   1866&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data set “D_meta” includes node information with students ID, class, and gender information.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(D_meta, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 3
##      ID Class Gender
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; 
## 1  1426 5B    M     
## 2  1427 5B    F     
## 3  1428 5B    M     
## 4  1429 5B    F     
## 5  1430 5B    M&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, I just created “frequency” column in the edgelist, “D” to give an edge attribute for each edges with their contact frequency, and deleted all edges having 0 contact.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#2. Manage dataset
B&amp;lt;-as.data.frame(table(D)) # Create an edge weight column named &amp;quot;Freq&amp;quot;
B1&amp;lt;-subset(B,Freq&amp;gt;0) # Delete all the edges having weight equal to 0
head(B1, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Source Target Freq
## 1     1426   1427   27
## 240   1426   1428   45
## 241   1427   1428    4
## 479   1426   1429   75
## 480   1427   1429  100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we can now create an igraph object. igraph object is different from the data frame and requires totally different grammar from the code handling general data frame. igraph is specially designed to handle large-sized complex network data with high speed. The igraph object, “Stucont” includes edgelist, nodelist, and their attributes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#3. Create an igraph object from the dataframes
Stucont&amp;lt;-graph_from_data_frame(B1, directed = FALSE, vertices = D_meta)
E(Stucont)$weight&amp;lt;-E(Stucont)$Freq # Assigning edge attribute to each edge
Stucont&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## IGRAPH 0a4ebbb UNW- 242 8317 -- 
## + attr: name (v/c), Class (v/c), Gender (v/c), Freq (e/n), weight (e/n)
## + edges from 0a4ebbb (vertex names):
##  [1] 1426--1427 1426--1428 1427--1428 1426--1429 1427--1429 1428--1429
##  [7] 1426--1430 1427--1430 1428--1430 1429--1430 1426--1431 1427--1431
## [13] 1428--1431 1429--1431 1430--1431 1426--1434 1427--1434 1428--1434
## [19] 1429--1434 1430--1434 1431--1434 1426--1435 1427--1435 1428--1435
## [25] 1429--1435 1430--1435 1431--1435 1434--1435 1426--1437 1427--1437
## [31] 1428--1437 1429--1437 1430--1437 1431--1437 1434--1437 1435--1437
## [37] 1426--1439 1427--1439 1428--1439 1429--1439 1430--1439 1431--1439
## [43] 1434--1439 1435--1439 1437--1439 1426--1441 1427--1441 1428--1441
## + ... omitted several edges&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-basic-network-features&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;Exploring basic network features&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;gsize shows us the number of edges, and gorder shows the number of nodes&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gsize(Stucont)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8317&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gorder(Stucont)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 242&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;V means the vertex (node), so with this function you can see the nodelist.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#2. Nodelist
V(Stucont)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## + 242/242 vertices, named, from 0a4ebbb:
##   [1] 1426 1427 1428 1429 1430 1431 1434 1435 1437 1439 1441 1443 1451 1452 1453
##  [16] 1457 1458 1459 1461 1465 1468 1471 1475 1477 1479 1480 1482 1483 1486 1489
##  [31] 1493 1495 1498 1500 1501 1502 1503 1504 1511 1516 1519 1520 1521 1522 1524
##  [46] 1525 1528 1532 1533 1538 1539 1545 1546 1548 1549 1551 1552 1555 1558 1560
##  [61] 1562 1563 1564 1567 1570 1572 1574 1578 1579 1580 1585 1592 1594 1601 1603
##  [76] 1604 1606 1609 1613 1616 1617 1618 1625 1628 1630 1632 1637 1641 1643 1647
##  [91] 1648 1649 1650 1653 1656 1661 1663 1664 1665 1666 1668 1670 1673 1674 1675
## [106] 1680 1681 1682 1684 1685 1687 1688 1695 1696 1697 1698 1700 1702 1704 1705
## [121] 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1718 1719 1720 1722 1723
## [136] 1727 1730 1731 1732 1735 1737 1738 1739 1741 1743 1744 1745 1746 1748 1749
## + ... omitted several vertices&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;E means the edge.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#3. Edgelist
E(Stucont)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## + 8317/8317 edges from 0a4ebbb (vertex names):
##  [1] 1426--1427 1426--1428 1427--1428 1426--1429 1427--1429 1428--1429
##  [7] 1426--1430 1427--1430 1428--1430 1429--1430 1426--1431 1427--1431
## [13] 1428--1431 1429--1431 1430--1431 1426--1434 1427--1434 1428--1434
## [19] 1429--1434 1430--1434 1431--1434 1426--1435 1427--1435 1428--1435
## [25] 1429--1435 1430--1435 1431--1435 1434--1435 1426--1437 1427--1437
## [31] 1428--1437 1429--1437 1430--1437 1431--1437 1434--1437 1435--1437
## [37] 1426--1439 1427--1439 1428--1439 1429--1439 1430--1439 1431--1439
## [43] 1434--1439 1435--1439 1437--1439 1426--1441 1427--1441 1428--1441
## [49] 1429--1441 1431--1441 1434--1441 1435--1441 1437--1441 1439--1441
## [55] 1426--1443 1427--1443 1428--1443 1429--1443 1430--1443 1431--1443
## + ... omitted several edges&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each node have attributes such as ID, class, and gender. There are missing values so we will change “unknown” into NA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#4. Attributes
V(Stucont)$Gender&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1] &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;      
##   [8] &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;      
##  [15] &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;      
##  [22] &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;      
##  [29] &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;      
##  [36] &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;      
##  [43] &amp;quot;Unknown&amp;quot; &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;      
##  [50] &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;      
##  [57] &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;      
##  [64] &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;      
##  [71] &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;      
##  [78] &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;Unknown&amp;quot; &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;      
##  [85] &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;Unknown&amp;quot; &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;      
##  [92] &amp;quot;M&amp;quot;       &amp;quot;Unknown&amp;quot; &amp;quot;Unknown&amp;quot; &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;      
##  [99] &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;Unknown&amp;quot; &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;      
## [106] &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;      
## [113] &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;      
## [120] &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;Unknown&amp;quot; &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;      
## [127] &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;      
## [134] &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;Unknown&amp;quot; &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;      
## [141] &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;Unknown&amp;quot;
## [148] &amp;quot;Unknown&amp;quot; &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;Unknown&amp;quot;
## [155] &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;      
## [162] &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;Unknown&amp;quot;
## [169] &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;      
## [176] &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;Unknown&amp;quot;
## [183] &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;      
## [190] &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;Unknown&amp;quot;
## [197] &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;      
## [204] &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;Unknown&amp;quot; &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;      
## [211] &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;      
## [218] &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;      
## [225] &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;      
## [232] &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;      
## [239] &amp;quot;F&amp;quot;       &amp;quot;M&amp;quot;       &amp;quot;F&amp;quot;       &amp;quot;F&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V(Stucont)$Gender[V(Stucont)$Gender==&amp;#39;Unknown&amp;#39;] &amp;lt;- NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The adjacency matrix is the most important indication of the network. However, igraph does not store the network data in the adjacency matrix format. Still, you can represent the network in the matrix format as below. We can know that the contact network is a symmetric and undirected network from the adjacency matrix, which means there is no arrow on edges.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#5. Adjacency matrix
Stucont[c(1:10),c(1:10)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10 x 10 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    [[ suppressing 10 column names &amp;#39;1426&amp;#39;, &amp;#39;1427&amp;#39;, &amp;#39;1428&amp;#39; ... ]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                     
## 1426  .  27 45  75 19 43  8 12 23 27
## 1427 27   .  4 100  4 63 20  5 44 13
## 1428 45   4  .   9  4 16  2  4 19 14
## 1429 75 100  9   .  9 75 11  5 62 36
## 1430 19   4  4   9  . 15  4  7  4  3
## 1431 43  63 16  75 15  . 43 16 42 41
## 1434  8  20  2  11  4 43  .  3  8  8
## 1435 12   5  4   5  7 16  3  .  6 11
## 1437 23  44 19  62  4 42  8  6  . 29
## 1439 27  13 14  36  3 41  8 11 29  .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;measuring-centrality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;Measuring centrality&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Centrality is the most important indicator of the network. The centrality measure shows which node has the highest contact frequency in the network. To understand more about the centrality measure, please visit &lt;a href=&#34;https://youtu.be/o5-o1EPSWZg&#34;&gt;my lecture page on the Youtube channel&lt;/a&gt;. I will not explain the details of each centrality measure in this post.&lt;/p&gt;
&lt;p&gt;I identified the nodes recording the highest score in each of the centrality measures and found that student 1551 shows the highest centrality in the degree and betweenness centrality measure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#1. Degree centrality
Stucont_deg&amp;lt;-degree(Stucont,mode=c(&amp;quot;All&amp;quot;))
V(Stucont)$degree&amp;lt;-Stucont_deg
which.max(Stucont_deg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1551 
##   56&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#2. Eigenvector centrality
Stucont_eig &amp;lt;- evcent(Stucont)$vector
V(Stucont)$Eigen&amp;lt;-Stucont_eig
which.max(Stucont_eig)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1665 
##   99&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#3. Betweenness centrality
Stucont_bw&amp;lt;-betweenness(Stucont, directed = FALSE)
V(Stucont)$betweenness&amp;lt;-Stucont_bw
which.max(Stucont_bw)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1551 
##   56&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;measuring-network-structure&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;Measuring network structure&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Each network has its unique structural features.&lt;/p&gt;
&lt;p&gt;First, network density indicates how much densely the nodes are connected in the network. Also, if you’d like to know more about the network theory, please watch my lecture on Youtube.&lt;/p&gt;
&lt;p&gt;Here I compared the network density between the school and the class level. It shows that the school level density is 0.28 while the class level density is 0.98. Thus, there is a considerable density gap between the school and the class.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#1. Network Density
edge_density(Stucont) # Global density&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2852097&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A1&amp;lt;-induced_subgraph(Stucont, V(Stucont)[Class==&amp;quot;1A&amp;quot;], impl=c(&amp;quot;auto&amp;quot;)) # Subgraphing into each class
edge_density(A1) # Class level density&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9841897&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also calculate the assortativity score, which means mingling together with the nodes having a similar attribute. For instance, we can expect that the same class students or having the same gender may have more frequent contact. The assortativity score of the class is 0.23.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#2. Assortativity
values &amp;lt;- as.numeric(factor(V(Stucont)$Class))
assortativity_nominal(Stucont, types=values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2337739&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But, we do not know how big enough or small to assess the level of assortativity. We can then create a random network, which has the same probability of having an edge between every node and comparing it. The histogram indicates that the school network data is such an abnormal case having a high assortativity score according to the random network’s probability distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#2.1. Calculate the observed assortativity
observed.assortativity &amp;lt;- assortativity_nominal(Stucont, types=values)
results &amp;lt;- vector(&amp;#39;list&amp;#39;, 1000)
for(i in 1:1000){results[[i]] &amp;lt;- assortativity_nominal(Stucont, sample(values))}
#2.2.  Plot the distribution of assortativity values and add a red vertical line at the original observed value
hist(unlist(results), xlim = c(0,0.4))
abline(v = observed.assortativity,col = &amp;quot;red&amp;quot;, lty = 3, lwd=2) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/aspb/2020-10-04-sdf.en_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;network-visualization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;Network Visualization&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The final step is the network visualization. The beauty of network analysis is that we can visually confirm the features we indicated by the numbers above. Also, the visual mapping of the network is instrumental in communicating with the audiences with data. Here, I set the size of each node with a value of degree centrality. Each different color indicates different classes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#1. Plotting a network with the degree centrality
set.seed(1001)
library(RColorBrewer) # This is the color library
pal&amp;lt;-brewer.pal(length(unique(V(Stucont)$Class)), &amp;quot;Set3&amp;quot;) # Vertex color assigned per each class number
plot(Stucont,edge.color = &amp;#39;black&amp;#39;,vertex.label.cex =0.5, 
     vertex.color=pal[as.numeric(as.factor(vertex_attr(Stucont, &amp;quot;Class&amp;quot;)))],
     vertex.size = sqrt(Stucont_deg)/2, edge.width=sqrt(E(Stucont)$weight/800),
     layout = layout.fruchterman.reingold)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/aspb/2020-10-04-sdf.en_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;community-detection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;Community Detection&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Based on the networking pattern of the node, we can cluster them into several groups. We can intuitively think that the nodes will be clustered based on their affiliation with the classes. However, the result is counter-intuitive. We have ten classes in the school dataset, but the number of detected communities (cluster) is 6, which means that this school is composed of 6 different sub-network groups.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#1. Louvain clustering
lc &amp;lt;- cluster_louvain(Stucont) # Create a cluster based on the Louvain method
communities(lc) # You can check which vertices belongs to which clusters.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $`1`
##  [1] &amp;quot;1426&amp;quot; &amp;quot;1427&amp;quot; &amp;quot;1428&amp;quot; &amp;quot;1429&amp;quot; &amp;quot;1430&amp;quot; &amp;quot;1431&amp;quot; &amp;quot;1434&amp;quot; &amp;quot;1435&amp;quot; &amp;quot;1437&amp;quot; &amp;quot;1439&amp;quot;
## [11] &amp;quot;1441&amp;quot; &amp;quot;1443&amp;quot; &amp;quot;1451&amp;quot; &amp;quot;1452&amp;quot; &amp;quot;1453&amp;quot; &amp;quot;1457&amp;quot; &amp;quot;1458&amp;quot; &amp;quot;1459&amp;quot; &amp;quot;1461&amp;quot; &amp;quot;1465&amp;quot;
## [21] &amp;quot;1468&amp;quot; &amp;quot;1471&amp;quot; &amp;quot;1475&amp;quot; &amp;quot;1477&amp;quot; &amp;quot;1479&amp;quot; &amp;quot;1480&amp;quot; &amp;quot;1482&amp;quot; &amp;quot;1483&amp;quot; &amp;quot;1486&amp;quot; &amp;quot;1489&amp;quot;
## [31] &amp;quot;1493&amp;quot; &amp;quot;1495&amp;quot; &amp;quot;1498&amp;quot; &amp;quot;1501&amp;quot; &amp;quot;1502&amp;quot; &amp;quot;1511&amp;quot; &amp;quot;1516&amp;quot; &amp;quot;1520&amp;quot; &amp;quot;1522&amp;quot; &amp;quot;1563&amp;quot;
## [41] &amp;quot;1578&amp;quot; &amp;quot;1585&amp;quot; &amp;quot;1592&amp;quot; &amp;quot;1637&amp;quot; &amp;quot;1668&amp;quot; &amp;quot;1750&amp;quot; &amp;quot;1751&amp;quot; &amp;quot;1824&amp;quot; &amp;quot;1885&amp;quot;
## 
## $`2`
##  [1] &amp;quot;1656&amp;quot; &amp;quot;1661&amp;quot; &amp;quot;1663&amp;quot; &amp;quot;1664&amp;quot; &amp;quot;1665&amp;quot; &amp;quot;1666&amp;quot; &amp;quot;1670&amp;quot; &amp;quot;1673&amp;quot; &amp;quot;1674&amp;quot; &amp;quot;1675&amp;quot;
## [11] &amp;quot;1680&amp;quot; &amp;quot;1681&amp;quot; &amp;quot;1682&amp;quot; &amp;quot;1684&amp;quot; &amp;quot;1687&amp;quot; &amp;quot;1688&amp;quot; &amp;quot;1695&amp;quot; &amp;quot;1696&amp;quot; &amp;quot;1697&amp;quot; &amp;quot;1698&amp;quot;
## [21] &amp;quot;1745&amp;quot; &amp;quot;1765&amp;quot; &amp;quot;1779&amp;quot; &amp;quot;1908&amp;quot; &amp;quot;1912&amp;quot; &amp;quot;1920&amp;quot;
## 
## $`3`
##  [1] &amp;quot;1711&amp;quot; &amp;quot;1752&amp;quot; &amp;quot;1753&amp;quot; &amp;quot;1757&amp;quot; &amp;quot;1759&amp;quot; &amp;quot;1760&amp;quot; &amp;quot;1761&amp;quot; &amp;quot;1764&amp;quot; &amp;quot;1766&amp;quot; &amp;quot;1767&amp;quot;
## [11] &amp;quot;1768&amp;quot; &amp;quot;1770&amp;quot; &amp;quot;1772&amp;quot; &amp;quot;1774&amp;quot; &amp;quot;1775&amp;quot; &amp;quot;1778&amp;quot; &amp;quot;1783&amp;quot; &amp;quot;1787&amp;quot; &amp;quot;1789&amp;quot; &amp;quot;1790&amp;quot;
## [21] &amp;quot;1792&amp;quot; &amp;quot;1796&amp;quot; &amp;quot;1798&amp;quot; &amp;quot;1799&amp;quot;
## 
## $`4`
##  [1] &amp;quot;1500&amp;quot; &amp;quot;1503&amp;quot; &amp;quot;1504&amp;quot; &amp;quot;1519&amp;quot; &amp;quot;1521&amp;quot; &amp;quot;1524&amp;quot; &amp;quot;1525&amp;quot; &amp;quot;1528&amp;quot; &amp;quot;1532&amp;quot; &amp;quot;1533&amp;quot;
## [11] &amp;quot;1538&amp;quot; &amp;quot;1539&amp;quot; &amp;quot;1545&amp;quot; &amp;quot;1546&amp;quot; &amp;quot;1548&amp;quot; &amp;quot;1549&amp;quot; &amp;quot;1601&amp;quot; &amp;quot;1618&amp;quot; &amp;quot;1630&amp;quot; &amp;quot;1632&amp;quot;
## [21] &amp;quot;1653&amp;quot; &amp;quot;1705&amp;quot; &amp;quot;1730&amp;quot; &amp;quot;1797&amp;quot; &amp;quot;1802&amp;quot; &amp;quot;1803&amp;quot; &amp;quot;1805&amp;quot; &amp;quot;1807&amp;quot; &amp;quot;1815&amp;quot; &amp;quot;1818&amp;quot;
## [31] &amp;quot;1819&amp;quot; &amp;quot;1821&amp;quot; &amp;quot;1831&amp;quot; &amp;quot;1835&amp;quot; &amp;quot;1836&amp;quot; &amp;quot;1837&amp;quot; &amp;quot;1847&amp;quot; &amp;quot;1857&amp;quot; &amp;quot;1865&amp;quot; &amp;quot;1866&amp;quot;
## [41] &amp;quot;1880&amp;quot; &amp;quot;1888&amp;quot; &amp;quot;1892&amp;quot; &amp;quot;1895&amp;quot; &amp;quot;1910&amp;quot;
## 
## $`5`
##  [1] &amp;quot;1603&amp;quot; &amp;quot;1604&amp;quot; &amp;quot;1606&amp;quot; &amp;quot;1609&amp;quot; &amp;quot;1613&amp;quot; &amp;quot;1616&amp;quot; &amp;quot;1617&amp;quot; &amp;quot;1625&amp;quot; &amp;quot;1628&amp;quot; &amp;quot;1641&amp;quot;
## [11] &amp;quot;1643&amp;quot; &amp;quot;1647&amp;quot; &amp;quot;1648&amp;quot; &amp;quot;1649&amp;quot; &amp;quot;1650&amp;quot; &amp;quot;1702&amp;quot; &amp;quot;1704&amp;quot; &amp;quot;1706&amp;quot; &amp;quot;1708&amp;quot; &amp;quot;1710&amp;quot;
## [21] &amp;quot;1713&amp;quot; &amp;quot;1715&amp;quot; &amp;quot;1718&amp;quot; &amp;quot;1732&amp;quot; &amp;quot;1739&amp;quot; &amp;quot;1743&amp;quot; &amp;quot;1749&amp;quot; &amp;quot;1851&amp;quot; &amp;quot;1852&amp;quot; &amp;quot;1854&amp;quot;
## [31] &amp;quot;1855&amp;quot; &amp;quot;1858&amp;quot; &amp;quot;1861&amp;quot; &amp;quot;1863&amp;quot; &amp;quot;1872&amp;quot; &amp;quot;1877&amp;quot; &amp;quot;1883&amp;quot; &amp;quot;1887&amp;quot; &amp;quot;1889&amp;quot; &amp;quot;1890&amp;quot;
## [41] &amp;quot;1897&amp;quot; &amp;quot;1898&amp;quot; &amp;quot;1902&amp;quot; &amp;quot;1906&amp;quot; &amp;quot;1907&amp;quot; &amp;quot;1911&amp;quot; &amp;quot;1913&amp;quot; &amp;quot;1916&amp;quot; &amp;quot;1917&amp;quot; &amp;quot;1919&amp;quot;
## [51] &amp;quot;1922&amp;quot;
## 
## $`6`
##  [1] &amp;quot;1551&amp;quot; &amp;quot;1552&amp;quot; &amp;quot;1555&amp;quot; &amp;quot;1558&amp;quot; &amp;quot;1560&amp;quot; &amp;quot;1562&amp;quot; &amp;quot;1564&amp;quot; &amp;quot;1567&amp;quot; &amp;quot;1570&amp;quot; &amp;quot;1572&amp;quot;
## [11] &amp;quot;1574&amp;quot; &amp;quot;1579&amp;quot; &amp;quot;1580&amp;quot; &amp;quot;1594&amp;quot; &amp;quot;1685&amp;quot; &amp;quot;1700&amp;quot; &amp;quot;1707&amp;quot; &amp;quot;1709&amp;quot; &amp;quot;1712&amp;quot; &amp;quot;1714&amp;quot;
## [21] &amp;quot;1719&amp;quot; &amp;quot;1720&amp;quot; &amp;quot;1722&amp;quot; &amp;quot;1723&amp;quot; &amp;quot;1727&amp;quot; &amp;quot;1731&amp;quot; &amp;quot;1735&amp;quot; &amp;quot;1737&amp;quot; &amp;quot;1738&amp;quot; &amp;quot;1741&amp;quot;
## [31] &amp;quot;1744&amp;quot; &amp;quot;1746&amp;quot; &amp;quot;1748&amp;quot; &amp;quot;1763&amp;quot; &amp;quot;1780&amp;quot; &amp;quot;1782&amp;quot; &amp;quot;1795&amp;quot; &amp;quot;1800&amp;quot; &amp;quot;1801&amp;quot; &amp;quot;1809&amp;quot;
## [41] &amp;quot;1820&amp;quot; &amp;quot;1822&amp;quot; &amp;quot;1833&amp;quot; &amp;quot;1838&amp;quot; &amp;quot;1843&amp;quot; &amp;quot;1859&amp;quot; &amp;quot;1909&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#2. Plotting the Betweenness Centrality network with the community detection

set.seed(1001) # To duplicate the computer process and create exactly the same network repetitively you should set the seed.
plot(lc, Stucont, edge.color = &amp;#39;black&amp;#39;,vertex.label.cex =0.5, 
     vertex.color=pal[as.numeric(as.factor(vertex_attr(Stucont, &amp;quot;Class&amp;quot;)))],
     vertex.size = sqrt(Stucont_bw)/3, edge.width=sqrt(E(Stucont)$weight/800),
     layout = layout.fruchterman.reingold)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/aspb/2020-10-04-sdf.en_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So far, we walked through the whole process of network analysis and visualization. Although this analysis is descriptive, we could learn a lot about the school’s student dynamics only with this small information. Also, it provided information on the latent community structure that was not visible before the analysis.&lt;/p&gt;
&lt;p&gt;The students’ physical contact data set is rare because it has a privacy issue to measure the contact information. Thus, this analysis provides us such a precious insight into the students’ physical network in school.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Temporal network simulation: Albert-Barabasi model</title>
      <link>/post/temporal-network-simulation-albert-barabasi-model/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/temporal-network-simulation-albert-barabasi-model/</guid>
      <description>


&lt;p&gt;By Byoung-gyu Gong&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-03-temporal-network-simulation-albert-barabasi-model.en_files/example961.png&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;theory&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Theory&lt;/h1&gt;
&lt;p&gt;Albert-Barabasi model is well-known network generation method, adding new vertices at each time steps. The connections added to the existing vertices at each time are proportional to the degree of the given vertice. The generated network shows the rich-get-richer phenomenon in the networked world as new connection tends to increase proportional to the previous degree.&lt;/p&gt;
&lt;p&gt;The master equation is as follows:
&lt;span class=&#34;math display&#34;&gt;\[P[i] = k[i]^a + b\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where k[i] is the in-degree of vertex i in the current time, a is an exponent to create a power-law distribution, and b is attractiveness of the vertices with no degree.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-script&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R script&lt;/h1&gt;
&lt;div id=&#34;albert-barabasi-network-creation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Albert-Barabasi network creation&lt;/h2&gt;
&lt;p&gt;We can create the Albert-Barabasi network data set using “sample_pa” function in igraph package. 100 means the number of vertices to create, power=3 is the ‘a’ in the above formula, and m is the number of edges to add for each time step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A&amp;lt;-sample_pa(100, power=3, m=2, directed=FALSE, algorithm = &amp;quot;psumtree&amp;quot;)
A&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## IGRAPH 73ee564 U--- 100 197 -- Barabasi graph
## + attr: name (g/c), power (g/n), m (g/n), zero.appeal (g/n), algorithm
## | (g/c)
## + edges from 73ee564:
##  [1] 1-- 2 1-- 3 2-- 3 1-- 4 2-- 4 2-- 5 1-- 5 2-- 6 1-- 6 2-- 7 1-- 7 2-- 8
## [13] 1-- 8 2-- 9 1-- 9 1--10 2--10 2--11 1--11 1--12 2--12 1--13 2--13 2--14
## [25] 1--14 2--15 1--15 1--16 2--16 1--17 2--17 2--18 1--18 1--19 2--19 1--20
## [37] 2--20 1--21 2--21 1--22 2--22 2--23 1--23 2--24 1--24 1--25 2--25 1--26
## [49] 2--26 1--27 2--27 1--28 2--28 2--29 1--29 2--30 1--30 2--31 1--31 2--32
## [61] 1--32 2--33 1--33 1--34 2--34 2--35 1--35 1--36 2--36 1--37 2--37 1--38
## [73] 2--38 1--39 2--39 1--40 2--40 2--41 1--41 1--42 2--42 1--43 2--43 1--44
## + ... omitted several edges&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above network data object does not include any information on the time step as an edge attribute (we need information on when the given edge will be added to generate the network). So, we need to create a time step information and merge it to the network data object A.&lt;/p&gt;
&lt;p&gt;T is the vector of the time step. The reason why the same number repeats twice is that we modeled the network to add 2 edges per each time step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;T&amp;lt;-rep(x=seq(2,99), each=2)
T&amp;lt;-append(1,T)
T&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1]  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9 10 10 11 11 12 12 13 13
##  [26] 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21 22 22 23 23 24 24 25 25 26
##  [51] 26 27 27 28 28 29 29 30 30 31 31 32 32 33 33 34 34 35 35 36 36 37 37 38 38
##  [76] 39 39 40 40 41 41 42 42 43 43 44 44 45 45 46 46 47 47 48 48 49 49 50 50 51
## [101] 51 52 52 53 53 54 54 55 55 56 56 57 57 58 58 59 59 60 60 61 61 62 62 63 63
## [126] 64 64 65 65 66 66 67 67 68 68 69 69 70 70 71 71 72 72 73 73 74 74 75 75 76
## [151] 76 77 77 78 78 79 79 80 80 81 81 82 82 83 83 84 84 85 85 86 86 87 87 88 88
## [176] 89 89 90 90 91 91 92 92 93 93 94 94 95 95 96 96 97 97 98 98 99 99&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we can merge the time step data into the network data. Now, we can find “time (e/n)” in the igraph object, which means that the time step information is now assigned as an edge attribute.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;E(A)$time&amp;lt;-T
V(A)$name&amp;lt;-V(A)
A&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## IGRAPH 73ee564 UN-- 100 197 -- Barabasi graph
## + attr: name (g/c), power (g/n), m (g/n), zero.appeal (g/n), algorithm
## | (g/c), name (v/n), time (e/n)
## + edges from 73ee564 (vertex names):
##  [1] 1-- 2 1-- 3 2-- 3 1-- 4 2-- 4 2-- 5 1-- 5 2-- 6 1-- 6 2-- 7 1-- 7 2-- 8
## [13] 1-- 8 2-- 9 1-- 9 1--10 2--10 2--11 1--11 1--12 2--12 1--13 2--13 2--14
## [25] 1--14 2--15 1--15 1--16 2--16 1--17 2--17 2--18 1--18 1--19 2--19 1--20
## [37] 2--20 1--21 2--21 1--22 2--22 2--23 1--23 2--24 1--24 1--25 2--25 1--26
## [49] 2--26 1--27 2--27 1--28 2--28 2--29 1--29 2--30 1--30 2--31 1--31 2--32
## [61] 1--32 2--33 1--33 1--34 2--34 2--35 1--35 1--36 2--36 1--37 2--37 1--38
## [73] 2--38 1--39 2--39 1--40 2--40 2--41 1--41 1--42 2--42 1--43 2--43 1--44
## + ... omitted several edges&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following script was directly copied from the blog of &lt;a href=&#34;http://estebanmoro.org/post/2015-12-21-temporal-networks-with-r-and-igraph-updated/&#34;&gt;Dr. Esteban Moro&lt;/a&gt;. You should set the right directory before implementing the below chunk of codes as it creates 100 pages of image in the directory folder.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#this version of the script has been tested on igraph 1.0.1
#load libraries
require(igraph,RcolorBrewer)
install.packages(&amp;quot;RColorBrewer&amp;quot;)
library(RColorBrewer)
#generate a cool palette for the graph (darker colors = older nodes)
YlOrBr.pal &amp;lt;- colorRampPalette(brewer.pal(8,&amp;quot;YlOrRd&amp;quot;))
#colors for the nodes are chosen from the very beginning
V(A)$color &amp;lt;- rev(YlOrBr.pal(vcount(A)))[as.numeric(V(A)$name)]

#time in the edges goes from 1 to 300. We kick off at time 3
ti &amp;lt;- 2
#remove edges which are not present
gt &amp;lt;- delete_edges(A,which(E(A)$time &amp;gt; ti))
# Generate first layout using graphopt with normalized coordinates. 
# This places the initially connected set of nodes in the middle. 
# If you use fruchterman.reingold it will place that initial set in the outer ring.
layout.old &amp;lt;- norm_coords(layout.graphopt(gt), 
                          xmin = -1, xmax = 1, ymin = -1, ymax = 1)
#total time of the dynamics
total_time &amp;lt;- max(E(A)$time)
#This is the time interval for the animation. In this case is taken to be 1/10
#of the time (i.e. 10 snapshots) between adding two consecutive nodes
dt &amp;lt;- 0.1
#Output for each frame will be a png with HD size 1600x900 :)
png(file=&amp;quot;example%03d.png&amp;quot;, width=1600,height=900)
#Time loop starts
for(time in seq(3,total_time,dt)){
  #remove edges which are not present
  gt &amp;lt;- delete_edges(A,which(E(A)$time &amp;gt; time))
  #with the new graph, we update the layout a little bit
  layout.new &amp;lt;- layout_with_fr(gt,coords=layout.old,niter=10,start.temp=0.05,grid=&amp;quot;nogrid&amp;quot;)
  #plot the new graph
  plot(gt,layout=layout.new,
       vertex.label=&amp;quot;&amp;quot;,vertex.size=1+2*log(degree(gt)),
       vertex.frame.color=V(A)$color,edge.width=1.5,
       asp=9/16,margin=-0.15)
  #use the new layout in the next round
  layout.old &amp;lt;- layout.new
}
dev.off()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once you created the png image files of the network for each time step, then you can create an animation using “ffmpeg” application. If you already installed ‘brew’ in your computer, then you can easily install the “ffmpeg” with very short code like this in your terminal:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;$ brew install ffmpeg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, you can create an network animation with the following code in your terminal:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ffmpeg -r 10 -i example%03d.png -b:v 20M output.mp4&lt;/code&gt;&lt;/pre&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/ePzu6xg975I&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>

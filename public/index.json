[{"authors":null,"categories":null,"content":" Byoung-gyu Gong is a Ph.D. candidate of the educational policy and evaluation program at Arizona State University. He studies the intersection of artificial intelligence(AI) and education. Using the citation network and topic modeling analysis method, he explores how this new technological advancement shapes new educational policy and practice ground. He is also interested in educational data analytics using educational \u0026amp; learning big data and international large-scale assessment data.\n","date":1554595200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1554595200,"objectID":"d33c1cd8bead9ce2a6229ddb7fdca4a5","permalink":"/author/byoung-gyu-gong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/byoung-gyu-gong/","section":"authors","summary":"Byoung-gyu Gong is a Ph.D. candidate of the educational policy and evaluation program at Arizona State University. He studies the intersection of artificial intelligence(AI) and education. Using the citation network and topic modeling analysis method, he explores how this new technological advancement shapes new educational policy and practice ground.","tags":null,"title":"Byoung-gyu Gong","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":" By Byoung-gyu Gong  Course lectures  Online workshops Introduction to Network Analysis and Visualization\n ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"e29e97fe77b8a03dd681fdd4d485dceb","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":" By Byoung-gyu Gong  Course lectures  Online workshops Introduction to Network Analysis and Visualization\n ","tags":null,"title":"Lecture lists","type":"docs"},{"authors":null,"categories":null,"content":"  Type: Online workshop lecture Event: Annual Conference of Comparative and International Education Society(CIES), Time: May, 2020 Webpage: Lecture data sources Purpose of workshop: The network analysis is one of the critical quantitative methods exploring the relationship and connections of various kinds. In particular, the network analysis is appealing because it provides a bird\u0026rsquo;s eye view to the audiences with its visualization of the highly sophisticated network structures. Its usage in the field of comparative and international education studies is expected to ever grow with the increase in the available data.In this background, this workshop is designed to introduce basic theory and practical method of network analysis and network data visualization. This workshop targets audiences who want to familiarize themselves with the network analysis and visualization method. There is no prerequisite for the participants, and this workshop will provide basic level knowledge and skills for the beginners of network analysis. After this lecture, participants will be able to  Understand how the data can be collected, archived, and structured for the network analysis    ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"2bd145e1edb7853f7837689216f0acd7","permalink":"/courses/example/2020-10-07-introduction/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/2020-10-07-introduction/","section":"courses","summary":"Type: Online workshop lecture Event: Annual Conference of Comparative and International Education Society(CIES), Time: May, 2020 Webpage: Lecture data sources Purpose of workshop: The network analysis is one of the critical quantitative methods exploring the relationship and connections of various kinds.","tags":null,"title":"Network Concept and Theory","type":"docs"},{"authors":null,"categories":null,"content":"  Type: Online workshop Event: Annual Conference of Comparative and International Education Society(CIES), Time: May, 2020 Webpage: Lecture data sources Purpose of workshop:The participants are required to install free software packages such as R \u0026amp; R Studio and Gephi and bring their laptop. After this lecture particiapnts will be able to:  Use network analysis and visualization software tools such as R Studio and Gephi Understand the basics of network theory and application to the various education studies    ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"171ab78c0d5dc14187c7df5b729fa3cb","permalink":"/courses/example/2020-10-07-network-analysis-with-igraph-and-gephi/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/2020-10-07-network-analysis-with-igraph-and-gephi/","section":"courses","summary":"Type: Online workshop Event: Annual Conference of Comparative and International Education Society(CIES), Time: May, 2020 Webpage: Lecture data sources Purpose of workshop:The participants are required to install free software packages such as R \u0026amp; R Studio and Gephi and bring their laptop.","tags":null,"title":"Network Data Analysis and Visualization","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"Bibliographic Data Analysis I am interested in collecting, processing, and analyzing large-sized web-based textual data. The bibliographic data retrievable from the web journal databases such as Web of Science (WoS) and Scopus is one of the textual data source I usually work with, comprised of author, institution, keyword, citation, and abstract information.\nMy Dissertation Research Project My dissertation research used almost 450,000 bibliographic data to see how the newly emerging technology of artificial intelligence (AI) interacts with the education at the research level. My research identified cross-referencing pattern of the two fields by analyzing the journal-journal citation network.\nImplication of the Research This study is meaningful in two ways. First this study quantified and visualized textual data to represent educational research field in the citation network space. In the featured picture above, I visualized cross-referencing between the two fields with the red-colored edges in the middle of the two clusters for the last 60 years in the right upper side of the figure. The graph on the left side indicates the interdisciplinary journals identified through community detection method. Each color represents cluster identified through the Louvain community detection method. The figure in the right-bottom side is the pane-view of the citation exchange between journals.\nSecond, this research provides a glimpse of what is the foundational knowledge-base ranges across AI and educational studies. It can be an essential background knowledge to design educational policy and program to enhance education’s AI-readiness. The findings implied that the computational and statistical representation of human and machine intelligence created a theoretical continuum between the two fields, precipitating interaction and cross-referencing. Also, the human-machine interaction studies are the bridge-researches connecting AI and educational studies. Such findings will provide policy implication predicting future research direction to advance educational science research.\n","date":1614643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614643200,"objectID":"1b4fe39be4412566b4875208172b4019","permalink":"/project/dissertation/","publishdate":"2021-03-02T00:00:00Z","relpermalink":"/project/dissertation/","section":"project","summary":"Dissertation research project using citation network analysis method.","tags":["Network Analysis"],"title":"Dissertation Research Project","type":"project"},{"authors":null,"categories":["R"],"content":"  1. Learning Analytics Learning analytics is a computational approach analyzing large-sized students’ learning and education data. It became pretty much popular as the online learning and digitized school administrative platforms produce an infinite stream of students’ data daily. Many higher education institutions actively seek learning analytics to promote data-driven decision-making and evidence-based institutional innovation to enhance students’ performance, retention, and administrative efficiency.\n 2. Example Data Analysis This post will analyze students’ academic achievement data to see whether the underrepresented student group is performing well enough compared with the other majority group across the time and subject area. The data was artificially and randomly created based on the preconfigured parameter distribution.\n2.1. Explorative Data Analysis 2.1.1 Visual Investigation: Histogram UG represents the Underrepresented Group with 1, and non-UG is the other majority students with 0. MATHs means the math subjects with 1 and non-math subjects with 0. In the histogram below, the y-axis means the number of student count, and the x-axis means GPA. The upper histogram is the students’ distribution in the non-math subject area, and the bottom histogram is that in the math subject area. The blue and red dotted line in the middle of the histogram indicates the mean GPA score of UG and non-UG group students. This figure provides us a glimpse of that first; there is a clear gap between UG and non-UG students across all subject areas. However, it is also indicative that the gap has been slightly narrower in the math subjects.\n### UG vs. Non.UG student performance mean UG_mean\u0026lt;-Overall_Comp %\u0026gt;% group_by(STDNT_GROUP3,MATHs) %\u0026gt;% summarise(mean=mean(GRD_PTS_PER_UNIT)) ### Histogram Visualization ggplot(Overall_Comp, aes(x=GRD_PTS_PER_UNIT)) + geom_histogram(aes(fill=STDNT_GROUP3),position=\u0026quot;dodge\u0026quot;, bins=40, binwidth = 0.3)+ facet_grid(rows = vars(MATHs), labeller = label_both) + labs(title=\u0026quot;GPA Distribution\u0026quot;, x=\u0026quot;GPA\u0026quot;, y=\u0026quot;Count\u0026quot;) + theme_minimal() + geom_vline(data=filter(UG_mean, MATHs==0), aes(xintercept=mean), color=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;),linetype=\u0026quot;dashed\u0026quot;,size=0.8) + geom_vline(data=filter(UG_mean, MATHs==1), aes(xintercept=mean), color=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;),linetype=\u0026quot;dashed\u0026quot;,size=0.8) + scale_fill_manual(name=\u0026quot;UG\u0026quot;,labels = c(\u0026quot;Non-UG\u0026quot;, \u0026quot;UG\u0026quot;),values = c(\u0026quot;#FFCB05\u0026quot;, \u0026quot;#00274C\u0026quot;)) + theme(plot.title = element_text(size=14, face=\u0026quot;bold.italic\u0026quot;), axis.title.x = element_text(size=14, face=\u0026quot;bold\u0026quot;), axis.title.y = element_text(size=14, face=\u0026quot;bold\u0026quot;), axis.text.x = element_text(size=10, face=\u0026quot;bold\u0026quot;), axis.text.y = element_text(size=10, face=\u0026quot;bold\u0026quot;))  2.1.2 Visual Investigation: Time Series Change The graph below shows how UG and non-UG students’ average GPA score changes across the term period. In the non-math courses the gap has been maintained while the gap has been continuously narrowed in the math courses. (Yey! It is excellent news to raise equitable future STEM workforce.)\n# Time series trend analysis Term_TS_UG\u0026lt;-Overall_Comp %\u0026gt;% select(TERM,Term.Descr,STDNT_GROUP3,GRD_PTS_PER_UNIT, MATHs) %\u0026gt;% group_by(TERM,Term.Descr,STDNT_GROUP3, MATHs) %\u0026gt;% summarise(mean=mean(GRD_PTS_PER_UNIT)) ggplot(Term_TS_UG, aes(x=TERM, y=mean, group=STDNT_GROUP3))+ scale_x_discrete(labels = unique(Term_TS_UG$Term.Descr))+ scale_color_manual(name=\u0026quot;UG\u0026quot;,labels = c(\u0026quot;Non-UG\u0026quot;, \u0026quot;UG\u0026quot;),values = c(\u0026quot;#FFCB05\u0026quot;, \u0026quot;#00274C\u0026quot;)) + theme_minimal() + geom_smooth(method=loess,aes(color=STDNT_GROUP3), size=1) + facet_grid(rows = vars(MATHs), labeller = label_both) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, size=8), plot.title = element_text(size=14, face=\u0026quot;bold.italic\u0026quot;), axis.title.x = element_text(size=14, face=\u0026quot;bold\u0026quot;), axis.title.y = element_text(size=14, face=\u0026quot;bold\u0026quot;), axis.text.y = element_text(size=10, face=\u0026quot;bold\u0026quot;)) + labs(title=\u0026quot;GPA Change Across Terms\u0026quot;, x=\u0026quot;Term\u0026quot;, y=\u0026quot;GPA Mean\u0026quot;) + lims(y=c(2,4))  2.1.3 Visual Investigation: Interaction I checked the interaction effect between UG membership and taking math classes. If there is a clear interaction, we should spot line-crossing or apparent convergence between the two lines. Here, the result is a bit unclear. We cannot estimate whether there is an interaction or not with this picture.\n# Interaction plot between CSP membership and Physics class ggplot(Overall_Comp, aes(x=MATHs, y=GRD_PTS_PER_UNIT, group=STDNT_GROUP3,color=STDNT_GROUP3)) + stat_summary(fun = mean, geom = \u0026quot;point\u0026quot;) + stat_summary(fun = mean, geom = \u0026quot;line\u0026quot;) + scale_color_manual(name=\u0026quot;UG\u0026quot;,labels = c(\u0026quot;Non-UG\u0026quot;, \u0026quot;UG\u0026quot;),values = c(\u0026quot;darkred\u0026quot;, \u0026quot;#00274C\u0026quot;)) + theme_classic2() + ylim(y=c(0,4))+ labs(title=\u0026quot;Interaction between Maths and UG\u0026quot;, x=\u0026quot;Maths\u0026quot;, y=\u0026quot;GPA Mean\u0026quot;) + theme(axis.text.x = element_text(size=14, face=\u0026quot;bold\u0026quot;), plot.title = element_text(size=14, face=\u0026quot;bold.italic\u0026quot;), axis.title.x = element_text(size=14, face=\u0026quot;bold\u0026quot;), axis.title.y = element_text(size=14, face=\u0026quot;bold\u0026quot;), axis.text.y = element_text(size=14, face=\u0026quot;bold\u0026quot;))   2.2. Linear Multiple Regression 2.2.1 Simplistic Model Then, I ran the regression. First, I made up the simplistic model, just adding UG-membership (STDNT_GROUP3) and math class (MATHs) as independent variables. The result shows that the UG-membership tends to be negatively associated with students’ GPA score and this relationship is statistically significant. It also explains that the students taking math classes tend to score less compared to their counterparts in the other subject classes. It is also statistically significant.\nLM2\u0026lt;-lm(GRD_PTS_PER_UNIT ~ STDNT_GROUP3 + MATHs, data=Overall_Comp) summary(LM2) ## ## Call: ## lm(formula = GRD_PTS_PER_UNIT ~ STDNT_GROUP3 + MATHs, data = Overall_Comp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0988 -0.3988 0.2012 0.6012 1.2978 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 3.0988039 0.0008969 3454.92 \u0026lt;2e-16 *** ## STDNT_GROUP31 -0.3280914 0.0027967 -117.31 \u0026lt;2e-16 *** ## MATHs1 -0.0685440 0.0028988 -23.65 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.9308 on 1296733 degrees of freedom ## Multiple R-squared: 0.01084, Adjusted R-squared: 0.01084 ## F-statistic: 7106 on 2 and 1296733 DF, p-value: \u0026lt; 2.2e-16  2.2.2 All Possible Confounders Counted Then, I added other predictors in our regression model because sometimes the statistical significance in the basic model diminishes due to the other predictors. Although the coefficient score has been slightly decreased, the negative association between GPA and UG-membership and GPA scores and math classes is statistically significant. Also, it indicates male students tend to score lower compared with female students. HSGPA means the high school GPA score, and it is positively associated with their university GPA score. GPAO is the GPA score in the other classes so far in the university. It shows a solid association with the current GPA score. However, it was revealed no interaction effect between UG-membership and math classes, though it has a negative coefficient. It means that underrepresented group students taking math classes do not under- nor outperform another student group.\nLM6\u0026lt;-lm(GRD_PTS_PER_UNIT ~ STDNT_GROUP3 + MATHs + SEX + HSGPA + GPAO + STDNT_GROUP3:MATHs, data=Overall_Comp) summary(LM6) ## ## Call: ## lm(formula = GRD_PTS_PER_UNIT ~ STDNT_GROUP3 + MATHs + SEX + ## HSGPA + GPAO + STDNT_GROUP3:MATHs, data = Overall_Comp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9002 -0.3140 0.1327 0.4534 3.9119 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -0.2396423 0.0047388 -50.570 \u0026lt;2e-16 *** ## STDNT_GROUP31 -0.1131594 0.0024416 -46.346 \u0026lt;2e-16 *** ## MATHs1 -0.0240013 0.0025247 -9.507 \u0026lt;2e-16 *** ## SEXM -0.0206531 0.0013716 -15.058 \u0026lt;2e-16 *** ## HSGPA 0.0346540 0.0005522 62.761 \u0026lt;2e-16 *** ## GPAO 1.0003087 0.0013613 734.803 \u0026lt;2e-16 *** ## STDNT_GROUP31:MATHs1 -0.0128971 0.0091333 -1.412 0.158 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.7773 on 1296729 degrees of freedom ## Multiple R-squared: 0.3103, Adjusted R-squared: 0.3103 ## F-statistic: 9.722e+04 on 6 and 1296729 DF, p-value: \u0026lt; 2.2e-16  2.2.3 Multicollinearity Check Multicollinearity indicates a degree of association among the independent variables. Once the score exceeds 7 or 9, the model is inefficient because it includes some redundant variables explained by the other independent variables. Fortunately, in this regression model, there is no multicollinearity among the independent variables.\ncar::vif(LM6) ## STDNT_GROUP3 MATHs SEX HSGPA ## 1.093532 1.088315 1.008777 1.011272 ## GPAO STDNT_GROUP3:MATHs ## 1.030711 1.154940  2.2.4 Coefficient Change Across Time What if we divide the data set according to their terms and establish regression modeling for each term? Would the coefficients be the same across the time? It is just beyond multiple regression, and it may request multilevel modeling, assuming each students’ score observation is embedded in each term, or we can add term as a categorical independent variable. Also, there is a way to include the interaction between the variables and the term.\nHowever, this may require more complicated data preprocessing and can be felt between multiple regression here. So, I just decided to provide simple visualization, giving us a glimpse of each coefficient’s time-series change. The graphs indicate the coefficient for each term, and the vertical lines indicate confidence intervals. It shows a birds-eye view of the change in coefficients across time.\nAlthough it is not a rigorous statistical method to figure out coefficient change across time, it provides us some great insight that can build up our next model.\nThe students’ general GPA continuously been improved across the terms as we can see in the intercept graph.\n The underrepresented students’ group has improved their academic performance although their score stays lower than the majority students’ group. The graph “STDNT_GROUP3” shows this upward trend very clearly.\n The students dramatically reversed the coefficient value from negative to positive for the math class. This means that in the past, students taking math classes got a lower score comparing with the students in the non-math classes, but these days this tendency was reversed.\n The high-school GPA (HSGPA) is still a meaningful predictor of the student’s GPA in the university, but its association level is lower than in the past.\n There seems to be a seasonal trend in the students’ GPA score, as seen from graphs of Intercept, SEXM, and GPAO.\n  We can see a constant up and down between fall and winter semester. It may indicate that our model has some missing independent variables in explaining the students’ academic performance.\nStu_Dist\u0026lt;-Overall_Comp %\u0026gt;% select(TERM, Term.Descr, MATHs, STDNT_GROUP3) %\u0026gt;% group_by(TERM,Term.Descr, MATHs, STDNT_GROUP3) %\u0026gt;% summarise(n=n()) %\u0026gt;% ungroup() %\u0026gt;% group_by(TERM,Term.Descr,MATHs) %\u0026gt;% mutate(prop=n/sum(n)) %\u0026gt;% mutate(MATHs=as.factor(MATHs)) # LM Summary by each term LM_Summary_TERM\u0026lt;-Overall_Comp %\u0026gt;% group_by(TERM) %\u0026gt;% do(tidy(lm(GRD_PTS_PER_UNIT ~ STDNT_GROUP3 + MATHs + SEX + HSGPA + GPAO + STDNT_GROUP3:MATHs, data=.),conf.int = TRUE)) plot_list=list() for(i in unique(LM_Summary_TERM$term)){ p\u0026lt;-ggplot(filter(LM_Summary_TERM,term==i), aes(x=TERM, y=estimate)) + scale_x_discrete(labels = unique(Stu_Dist$Term.Descr)) + geom_line() + geom_point() + geom_errorbar(aes(ymax = conf.high, ymin=conf.low), width=0.2) + theme_bw() + facet_grid(rows=\u0026quot;term\u0026quot;) + theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) + geom_vline(aes(xintercept=0), color=c(\u0026quot;red\u0026quot;),linetype=\u0026quot;dashed\u0026quot;,size=0.8) plot_list[[i]]=p } ggarrange(plot_list[[1]],plot_list[[2]],plot_list[[3]],plot_list[[4]], plot_list[[5]],plot_list[[6]],plot_list[[7]], ncol = 1, nrow = 7)    3. Conclusion In this post, I examined the academic performance of the underrepresented students group comparing with another non-underrepresented students group in all courses and also in the math courses. My analysis discovered that the underrepresented students group tends to score lower both in all classes and math classes, but the performance gap has been narrowed down across the time. Also, the analysis found that the underrepresented students group does not specifically under-perform in the math classes comparing with the other classes.\nThis information can be crucial to make an institutional level policy decision. Fortunately, the result showed that underrepresented students group is improving their academic performance a lot, and even the influence of high school GPA score has been decrease, which means the students’ learning experience in the university contributed to their GPA score. However, it should be noticed that we may need more rigorous test to prove that the students’ performance has been much improved and the gap has been narrowed down across the time. I will address this problem in another posting.\n ","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614657702,"objectID":"0b1fa8863d4c4f924e18b1b25ba60a17","permalink":"/post/learning-analytics/learning-analytics-multiple-regression-analysis/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/post/learning-analytics/learning-analytics-multiple-regression-analysis/","section":"post","summary":"1. Learning Analytics Learning analytics is a computational approach analyzing large-sized students’ learning and education data. It became pretty much popular as the online learning and digitized school administrative platforms produce an infinite stream of students’ data daily.","tags":["Regression"],"title":"Learning Analytics: Is Underrepresented Students Group Performing Well?","type":"post"},{"authors":[],"categories":["Hierchical linear model (HLM)","Multilevel analysis","R"],"content":" 1. What is Nested Data Not all data is created to be independent. We often find that the real world data does not meet the strict statistical presumption that each observation should be independent and identically distributed, so-called iid. In many cases of social science studies, the observations sampled from a population tend to correlate because they belong to the same group, region, and culture. Sample data having such common and correlative trait is called nested data.\nFor instance, students in the same school share some traits derived from the school and district features. Students in the same school or district tend to have similar socio-economic status (SES) because students are assigned to the schools based on their residential location. Students in the same school also tend to share the same teachers, school leadership, and school resource. In this account, student samples in the same schools may violate iid principle correlating with each other.\n  2. Example Data Analysis It can be costly to ignore the nested data structure. This post will show you a difference between the models considering and not considering the nested data structure.\nI used the international students’ data file for this analysis. The students studied in a foreign country through a one-year study abroad program to raise intercultural understanding. I analyzed to what extent the self-efficacy level predicts or explains the level of students’ intercultural understanding.\n2.1. Explorative Data Analysis This is the summary of the data set. It tells us there are four variables. Intercultural_Understanding and Self-Efficacy are the continuous variables while the rest of Major and Year are categorical variables.\n# Summary of the data summary(Studyabroad) ## Intercultural_Understanding Self_Efficacy Major Year ## Min. :1.000 Min. :3.000 Eng :20 18-19:31 ## 1st Qu.:3.250 1st Qu.:4.208 Noneng:52 19-20:41 ## Median :4.333 Median :5.190 ## Mean :4.089 Mean :4.980 ## 3rd Qu.:5.000 3rd Qu.:5.762 ## Max. :6.000 Max. :6.000 ## NA\u0026#39;s :2 NA\u0026#39;s :1 To get a more detailed sense of the data, let’s check the distribution of the data with visualization.\n# Scatter plot SA \u0026lt;- ggscatter(Studyabroad, x = \u0026quot;Self_Efficacy\u0026quot;, y = \u0026quot;Intercultural_Understanding\u0026quot;,color = \u0026quot;#00AFBB\u0026quot;, size = 1, alpha = 0.6, add = \u0026quot;reg.line\u0026quot;, add.params = list(color = \u0026quot;blue\u0026quot;, fill = \u0026quot;lightgray\u0026quot;), conf.int = TRUE, cor.coef = TRUE ,rug = TRUE)+ border() # Marginal density plot of x (top panel) and y (right panel) xplot \u0026lt;- ggdensity(Studyabroad, \u0026quot;Self_Efficacy\u0026quot;, fill = \u0026quot;lightgrey\u0026quot;) yplot \u0026lt;- ggdensity(Studyabroad, \u0026quot;Intercultural_Understanding\u0026quot;, fill = \u0026quot;lightgrey\u0026quot;)+ rotate() # Cleaning the plots SA \u0026lt;- SA + rremove(\u0026quot;legend\u0026quot;) yplot \u0026lt;- yplot + clean_theme() + rremove(\u0026quot;legend\u0026quot;) xplot \u0026lt;- xplot + clean_theme() + rremove(\u0026quot;legend\u0026quot;) library(cowplot) plot_grid(xplot, NULL, SA, yplot, ncol = 2, align = \u0026quot;hv\u0026quot;, rel_widths = c(2, 1), rel_heights = c(1, 2))  2.2. Simple Linear Regression The easiest way to analyze this data is simple linear regression. The analysis summary indicates that the students’ self-efficacy is positively associated with the level of intercultural understanding. The coefficient estimate is 0.3914, and the p-value is significant at the 0.05 level.\nLm0\u0026lt;-lm(Intercultural_Understanding ~ Self_Efficacy, data = Studyabroad) summary(Lm0) ## ## Call: ## lm(formula = Intercultural_Understanding ~ Self_Efficacy, data = Studyabroad) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.9404 -0.9175 0.2527 0.9850 1.8409 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 2.1298 0.9314 2.287 0.0254 * ## Self_Efficacy 0.3914 0.1841 2.126 0.0372 * ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 1.253 on 67 degrees of freedom ## (3 observations deleted due to missingness) ## Multiple R-squared: 0.06319, Adjusted R-squared: 0.04921 ## F-statistic: 4.52 on 1 and 67 DF, p-value: 0.0372  2.3. Alternative Analytic Methods for the Nested Data However, the data is stratified with other grouping variables. The data ‘Studyabroad’ has a ‘Major’ variable, indicating that each student belongs to a specific major program. The students’ intercultural understanding explained by self-efficacy may be influenced by their majors.\nWe can plot the regression line in the above scatter plot, but I will draw two different lines for each group this time. The scatter plot below shows that students in English and non-English majors have very different patterns of association. In general, the students’ intercultural understanding in English majors is related to their self-efficacy level, while the non-English major students showed no relationship between them.\nSA\u0026lt;-ggscatter(Studyabroad, x = \u0026quot;Self_Efficacy\u0026quot;, y = \u0026quot;Intercultural_Understanding\u0026quot;, size = 0.3, combine = TRUE, color = \u0026quot;Major\u0026quot;, palette = \u0026quot;jco\u0026quot;, add = \u0026quot;reg.line\u0026quot;, conf.int = TRUE) + stat_cor(aes(color = Major), method = \u0026quot;spearman\u0026quot;) # Marginal density plot of x (top panel) and y (right panel) xplot \u0026lt;- ggdensity(Studyabroad, \u0026quot;Self_Efficacy\u0026quot;, fill = \u0026quot;Major\u0026quot;, palette = \u0026quot;jco\u0026quot;) yplot \u0026lt;- ggdensity(Studyabroad, \u0026quot;Intercultural_Understanding\u0026quot;, fill = \u0026quot;Major\u0026quot;,palette = \u0026quot;jco\u0026quot;)+ rotate() # Cleaning the plots SA \u0026lt;- SA + rremove(\u0026quot;legend\u0026quot;) yplot \u0026lt;- yplot + clean_theme() + rremove(\u0026quot;legend\u0026quot;) xplot \u0026lt;- xplot + clean_theme() + rremove(\u0026quot;legend\u0026quot;) library(cowplot) plot_grid(xplot, NULL, SA, yplot, ncol = 2, align = \u0026quot;hv\u0026quot;, rel_widths = c(2, 1), rel_heights = c(1, 2)) The biggest problem of not considering the nested structure of the data is underestimating each parameter estimation’s standard error. This underestimation is fed into a more frequent rejection of the null hypothesis for the coefficient estimation. We have already seen that the simple linear regression model rejected the null hypothesis above.\n2.3.1. Solution1. Complex Linear Regression Model with Grouping Dummy Variables One of the immediate solutions to handle nested data issue is adding grouping dummy variables in the regression modeling.\nThe new regression model below included ‘Major,’ one of the grouping variables, in the modeling. Comparing with the previous simple linear regression, the standard error of Self-efficacy’s coefficient increased from 0.1841 to 0.2565. Moreover, the significant test of the coefficient estimation did not reject the null hypothesis.\nLm1\u0026lt;-lm(Intercultural_Understanding ~ Self_Efficacy + Major, data = Studyabroad) summary(Lm1) ## ## Call: ## lm(formula = Intercultural_Understanding ~ Self_Efficacy + Major, ## data = Studyabroad) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.93139 -0.89425 0.05834 1.09501 1.70774 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 2.7438 1.0816 2.537 0.0136 * ## Self_Efficacy 0.1925 0.2565 0.750 0.4557 ## MajorNoneng 0.5228 0.4706 1.111 0.2706 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 1.251 on 66 degrees of freedom ## (3 observations deleted due to missingness) ## Multiple R-squared: 0.08039, Adjusted R-squared: 0.05253 ## F-statistic: 2.885 on 2 and 66 DF, p-value: 0.06293  2.3.2. Solution2. Hierarchical Linear Modeling (HLM) or Multilevel Modeling Then, at this time, I will use hierarchical linear modeling analysis with the lme4 package to compare it with the previous simple linear regression model.\nFirst, I will create a null model to calculate the intraclass correlation (ICC). This number indicates the proportion of the outcome variable’s total variation explained by between-group variation. The ICC calculation for this data is about 12%, meaning that the students’ difference in affiliation to majors explains the 12% out of the total variation of the intercultural understanding.\nlibrary(lme4) Lme0\u0026lt;-lmer(Intercultural_Understanding ~ 1+(1|Major), data=Studyabroad) summary(Lme0) ## Linear mixed model fit by REML [\u0026#39;lmerMod\u0026#39;] ## Formula: Intercultural_Understanding ~ 1 + (1 | Major) ## Data: Studyabroad ## ## REML criterion at convergence: 231.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.2609 -0.6583 0.1094 0.8609 1.4064 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Major (Intercept) 0.2082 0.4563 ## Residual 1.5430 1.2422 ## Number of obs: 70, groups: Major, 2 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 3.9655 0.3606 10.99 # ICC Calculation library(sjstats) icc(Lme0) ## # Intraclass Correlation Coefficient ## ## Adjusted ICC: 0.119 ## Conditional ICC: 0.119 Then, I added self-efficacy as a predictor. The summary statistics indicate that the self-efficacy coefficient is not significant due to its increased standard error estimation. This tells us that similar to the regression model with the grouping dummy variables, the multilevel modeling also avoids underestimating the standard error lowering probability to reject the null hypothesis.\nlibrary(lmerTest) Lme1\u0026lt;-lmer(Intercultural_Understanding ~ 1 + Self_Efficacy + (1+Self_Efficacy|Major), data=Studyabroad, na.action = na.exclude) summary(Lme1) ## Linear mixed model fit by REML. t-tests use Satterthwaite\u0026#39;s method [ ## lmerModLmerTest] ## Formula: ## Intercultural_Understanding ~ 1 + Self_Efficacy + (1 + Self_Efficacy | ## Major) ## Data: Studyabroad ## ## REML criterion at convergence: 223.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.4245 -0.5919 0.1211 0.8689 1.4253 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Major (Intercept) 34.365 5.862 ## Self_Efficacy 1.717 1.310 -1.00 ## Residual 1.422 1.193 ## Number of obs: 69, groups: Major, 2 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(\u0026gt;|t|) ## (Intercept) 0.2977 4.2955 0.6712 0.069 0.960 ## Self_Efficacy 0.8874 0.9615 0.6535 0.923 0.582 ## ## Correlation of Fixed Effects: ## (Intr) ## Self_Effccy -0.999 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular    3. Conclusion This post addressed analyzing nested data, which often violates iid assumption for the inferential statistics. I compared the simple regression model with a more complex regression model, including the group dummy variable and multilevel model. This comparison showed that the simple regression model without considering nested data structure tends to underestimate the coefficient’s standard error, thus more frequently rejecting the null hypothesis in the significant test. Thus, to avoid this underestimation of standard error, it is generally recommended to use a more complex regression model with group dummy variables or multilevel modeling analysis.\n ","date":1607558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607639493,"objectID":"d8e18d5b867a224182591efc24b0e1d9","permalink":"/post/nested-data/analyzing-nested-data-comparing-multiple-approaches/","publishdate":"2020-12-10T00:00:00Z","relpermalink":"/post/nested-data/analyzing-nested-data-comparing-multiple-approaches/","section":"post","summary":"1. What is Nested Data Not all data is created to be independent. We often find that the real world data does not meet the strict statistical presumption that each observation should be independent and identically distributed, so-called iid.","tags":["lme4","R"],"title":"Analyzing Nested Data: Comparing Multiple Approaches","type":"post"},{"authors":[],"categories":["Educational data analytics","International large-scale assessment","Multilevel analysis"],"content":" By Byoung-gyu Gong\n  1. What is ILSA and PISA International large-scale assessment (ILSA) is conducted regularly by international organizations to measure and compare many different countries’ educational performance and status. The Programme for International Student Assessment (PISA) by OECD is the most well-known ILSA testing member and non-member partner country’s 15-year-old students every two years. It measures students’ reading, math, and science using a computer-based instrument. It also surveys to collect more detailed background information of each student, teacher, and school. The PISA 2006 collected this massive size of data from 398,750 students. Thus, it can be said that the PISA is the most comprehensive large-sized international student assessment data set available now.\n  2. Why is it difficult to analyze PISA data Despite the insightful knowledge we can gain from this large international students’ data set, PISA is not easily accessible to most of the researchers to analyze it. That is not because the data access is limited but because it is complexly structured, requesting sophisticated data pre-processing. Even Jerrim, Lopez-Agudo, Marcenaro-Gutierrez, and Shure (2017) said, “in spite of their acquired relevance, there are few studies which really account for the complex survey and test designs that they present and follow the technical procedures suggested by their developers” (p.1). I will illustrate what caution is needed to analyze the data set and provides R solution in this post using a data set of PISA 2006. 2.1. Two-staged sampling The sampling design of the PISA is the reason why the analysis procedure is so complicated. The PISA does not use random sampling and instead follows two-staged sampling: sample schools and then sample students in the participating schools. It makes the sampling errors of population estimates increase. It conflicts with most computer software designed to assume random sampling for statistical analysis and requests more sophisticated analysis procedures. Once the assumption of random sampling is violated, the student data can depend on each other because they may share common school characteristics.\nTo avoid such bias, PISA made some complementary measures. Once you look at the data table imported from the PISA 2006, you would see some strange variable names such as PV1-5 for each reading, math, and science domains. Also, there are some variables having names like W_FSTR1-80. If you just naively wanted to calculate students’ mean or correlation coefficient with a fixed point score, you would be embarrassed to find these unknown variables.\n# Use the following packages for the analysis library(intsvy) library(dplyr) library(ggplot2) head(pisa2006,1) ## X CNT SCHOOLID STIDSTD PV1MATH PV2MATH PV3MATH PV4MATH PV5MATH PV1READ ## 1 1 ARG 1 1 305.1799 305.9589 264.6752 320.7587 301.2852 357.0519 ## PV2READ PV3READ PV4READ PV5READ PV1SCIE PV2SCIE PV3SCIE PV4SCIE ## 1 312.1348 281.6554 316.9474 316.1453 395.3131 411.1652 445.6667 351.4869 ## PV5SCIE PV1INTR PV2INTR PV3INTR PV4INTR PV5INTR PV1SUPP PV2SUPP ## 1 356.1492 539.9939 480.1177 497.9912 370.1956 506.0343 339.0228 395.3084 ## PV3SUPP PV4SUPP PV5SUPP PV1EPS PV2EPS PV3EPS PV4EPS PV5EPS ## 1 412.5387 313.7517 322.9412 393.4482 384.1234 354.2843 430.7471 417.6925 ## PV1ISI PV2ISI PV3ISI PV4ISI PV5ISI PV1USE PV2USE PV3USE ## 1 334.7023 304.8632 323.5126 399.9755 386.9209 401.8404 383.191 385.0559 ## PV4USE PV5USE W_FSTUWT W_FSTR1 W_FSTR2 W_FSTR3 W_FSTR4 W_FSTR5 W_FSTR6 ## 1 436.342 469.911 98.8278 52.1041 139.9112 52.1041 139.9112 52.1041 139.9112 ## W_FSTR7 W_FSTR8 W_FSTR9 W_FSTR10 W_FSTR11 W_FSTR12 W_FSTR13 W_FSTR14 W_FSTR15 ## 1 52.1041 52.1041 52.1041 52.1041 139.9112 139.9112 52.1041 139.9112 139.9112 ## W_FSTR16 W_FSTR17 W_FSTR18 W_FSTR19 W_FSTR20 W_FSTR21 W_FSTR22 W_FSTR23 ## 1 52.1041 52.1041 139.9112 139.9112 139.9112 52.1041 139.9112 52.1041 ## W_FSTR24 W_FSTR25 W_FSTR26 W_FSTR27 W_FSTR28 W_FSTR29 W_FSTR30 W_FSTR31 ## 1 139.9112 52.1041 139.9112 52.1041 52.1041 52.1041 52.1041 139.9112 ## W_FSTR32 W_FSTR33 W_FSTR34 W_FSTR35 W_FSTR36 W_FSTR37 W_FSTR38 W_FSTR39 ## 1 139.9112 52.1041 139.9112 139.9112 52.1041 52.1041 139.9112 139.9112 ## W_FSTR40 W_FSTR41 W_FSTR42 W_FSTR43 W_FSTR44 W_FSTR45 W_FSTR46 W_FSTR47 ## 1 139.9112 52.1041 139.9112 52.1041 139.9112 52.1041 139.9112 52.1041 ## W_FSTR48 W_FSTR49 W_FSTR50 W_FSTR51 W_FSTR52 W_FSTR53 W_FSTR54 W_FSTR55 ## 1 52.1041 52.1041 52.1041 139.9112 139.9112 52.1041 139.9112 139.9112 ## W_FSTR56 W_FSTR57 W_FSTR58 W_FSTR59 W_FSTR60 W_FSTR61 W_FSTR62 W_FSTR63 ## 1 52.1041 52.1041 139.9112 139.9112 139.9112 52.1041 139.9112 52.1041 ## W_FSTR64 W_FSTR65 W_FSTR66 W_FSTR67 W_FSTR68 W_FSTR69 W_FSTR70 W_FSTR71 ## 1 139.9112 52.1041 139.9112 52.1041 52.1041 52.1041 52.1041 139.9112 ## W_FSTR72 W_FSTR73 W_FSTR74 W_FSTR75 W_FSTR76 W_FSTR77 W_FSTR78 W_FSTR79 ## 1 139.9112 52.1041 139.9112 139.9112 52.1041 52.1041 139.9112 139.9112 ## W_FSTR80 ESCS ## 1 139.9112 -2.0048 Then, what are these variables in the PISA table?  2.2. Sampling Weight To make each sampled student from each school represent each country’s entire population, we need to give a weight to each student variables. The random sample secures each sampled student’s equal probability, but two-stage sampling needs to adjust the probability with a complicated calculation of the weight. Weight is an inverse of probability to be selected as a sample. For instance, we selected ten students out of 100 students, then the probability to be selected is 0.1, and the weight is 10. In PISA, each school’s probability to be selected is proportional to its size, which is called PPS sampling method. The schools with larger sizes have a higher probability of selection, but the larger schools have a proportionally less within-school probability of selection. The sum of students’ weight for each country is the total number of the students’ population assumed for the study. There are two options to apply students’ weight for the PISA analysis, applying final weight(W_FSTUWT), and replicated weight(F_FSTR1-80), but in this post, I do not tackle the replicated weight because it is more related to the estimation of standard error, but not have an impact to the mean, correlation, and regression coefficent estimate. Let me show you an example here with R code. W_FSTUWT is the final students’ weight. You can see that students in the same school have approximately the same school weights with a little variation. School number 00001 has weights around 98, and 00002 has it at about 89.\n# Select Argentina students\u0026#39; weight information. WEIGHT\u0026lt;-pisa2006 %\u0026gt;% select(CNT, SCHOOLID, W_FSTUWT) %\u0026gt;% filter(CNT==\u0026quot;ARG\u0026quot;) # Create a dataframe that can compare Argentina students in three different schools A\u0026lt;-WEIGHT %\u0026gt;% filter(SCHOOLID==\u0026quot;1\u0026quot;) %\u0026gt;% slice(1:3) B\u0026lt;-WEIGHT %\u0026gt;% filter(SCHOOLID==\u0026quot;2\u0026quot;) %\u0026gt;% slice(1:3) C\u0026lt;-WEIGHT %\u0026gt;% filter(SCHOOLID==\u0026quot;3\u0026quot;) %\u0026gt;% slice(1:3) rbind(A,B,C) ## CNT SCHOOLID W_FSTUWT ## 1 ARG 1 98.8278 ## 2 ARG 1 91.7075 ## 3 ARG 1 98.8278 ## 4 ARG 2 89.3573 ## 5 ARG 2 89.6970 ## 6 ARG 2 89.6970 ## 7 ARG 3 112.7806 ## 8 ARG 3 117.0837 ## 9 ARG 3 117.0837  The summation of the students’ weight is equal to the population total population of 15-years-old students in Argentina. It is 523,047.\nWEIGHT %\u0026gt;% filter(CNT==\u0026quot;ARG\u0026quot;) %\u0026gt;% group_by(CNT) %\u0026gt;% summarize(sum=sum(W_FSTUWT)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 1 x 2 ## CNT sum ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ARG 523048.  Also, we can compare the difference in variable means between raw scores and weighted scores. Comparing with the graph ‘B’ with the graph ‘A,’ we can recognize that the graph ‘B’ is more slightly deviated from the regression line, although the difference is negligible for most countries. Some of the countries in specific variables, the bias to the estimate can be significantly large so we should apply the final student weight for the analysis all the time.\n# Create weighted mean score (W_MEAN) and non-weighted means score of PV1SCIE (science score) # Calculate national mean PV1SCIE\u0026lt;-pisa2006 %\u0026gt;% select(CNT, PV1SCIE, W_FSTUWT) %\u0026gt;% group_by(CNT) %\u0026gt;% mutate(W_MEAN=weighted.mean(PV1SCIE,W_FSTUWT, na.rm=TRUE), MEAN=mean(PV1SCIE, na.rm=TRUE)) %\u0026gt;% ungroup() %\u0026gt;% select(CNT, W_MEAN, MEAN) %\u0026gt;% unique() # Create a scatter plot to compare the discrepancy between library(ggplot2) library(ggpubr) # Create a scatter plot with raw score means (unweighted mean) Non_weight\u0026lt;-ggplot(PV1SCIE, aes(x=MEAN, y=MEAN)) + geom_point() + geom_text(data=PV1SCIE, aes(label=CNT), position=position_jitter(width=0.1,height=0.1), size=3) + geom_smooth(method=lm, se=TRUE)+stat_cor(method = \u0026quot;pearson\u0026quot;) # Create a scatter plot with unweighted mean and weighted mean Weight_non_weight\u0026lt;-ggplot(PV1SCIE, aes(x=MEAN, y=W_MEAN)) + geom_point() + geom_text(data=PV1SCIE, aes(label=CNT), position=position_jitter(width=0.1,height=0.1), size=3) + geom_smooth(method=lm, se=TRUE)+stat_cor(method = \u0026quot;pearson\u0026quot;) # Compore these two ggarrange(Non_weight, Weight_non_weight, labels = c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;), ncol = 2, nrow = 1) ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39; ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;  2.3. Plausible Value(PV) PV is called plausible value, and each domain has five PVs. For instance, math has PV1MATH - PV5MATH, and science has PV1SCIE - PV5SCIE. The PVs are introduced here to adjust measurement error that can be caused by - the concept to be measured is not clear, - students physical and psychological condition affected at the moment of testing, and - the testing environment on a day of testing. Thus, if we have five different plausible values distributed in a certain range of raw scores, we can avoid point estimate of the students’ competency, which has a high risk of measurement error as mentioned above. It means that we interpret students’ competency as a probability distribution among the plausible values, not as a fixed point. In addition to this rational, the PV was adopted in a practical and efficient standpoint. In PISA tests, students do not take all the test items as it is time-consuming, and each student takes different sets of tests, while the test items missing in each student are treated as a missing value. It creates five different sets of literally plausible score values of the students. I created two columns of mean data to compare weighted PV means and weighted means of each country. However, as we can see from the below table, there is no significant difference between the weighted PV mean and just the weighted mean with the final weight. The OECD also admitted that PV value does not bring significant impact to reduce bias in the estimate (OECD, 2009)\n# Create a dataframe with weighted PV means (weighted by final weight) PVSCIE\u0026lt;-pisa.mean.pv(pvlabel=\u0026quot;SCIE\u0026quot;, by=\u0026quot;CNT\u0026quot;, data=pisa2006) # Create a dataframe with weighted means (weighted by final weight) SCIE\u0026lt;-pisa2006 %\u0026gt;% group_by(CNT) %\u0026gt;% summarise(Mean=(weighted.mean(PV4SCIE,W_FSTUWT, na.rm=TRUE))) %\u0026gt;% mutate(Mean=round(Mean, 2)) ## `summarise()` ungrouping output (override with `.groups` argument) Merge\u0026lt;-left_join(PVSCIE, SCIE, by=\u0026quot;CNT\u0026quot;) %\u0026gt;% select(CNT, Freq, Mean.x, Mean.y) %\u0026gt;% rename(PVmean=\u0026quot;Mean.x\u0026quot;, Rawmean=\u0026quot;Mean.y\u0026quot;) head(Merge,15) ## CNT Freq PVmean Rawmean ## 1 ARG 4339 391.24 391.33 ## 2 AUS 14170 526.88 526.90 ## 3 AUT 4927 510.84 510.95 ## 4 AZE 5184 382.33 381.78 ## 5 BEL 8857 510.36 510.62 ## 6 BGR 4498 434.08 433.27 ## 7 BRA 9295 390.33 389.43 ## 8 CAN 22646 534.47 534.28 ## 9 CHE 12192 511.52 511.71 ## 10 CHL 5233 438.18 438.26 ## 11 COL 4478 388.04 388.60 ## 12 CZE 5932 512.86 512.69 ## 13 DEU 4891 515.65 516.32 ## 14 DNK 4532 495.89 495.05 ## 15 ESP 19604 488.42 487.51    3. Conclusion As we examined so far, the students’ final weight is a significant factor to reduce bias in estimation. Although PV value has less impact to the estimation and OECD also admit it, but still it should be best to follow the PISA instruction.\nIn this post, I only explored the estimates at each country level. However, it is very different for the case of cross-national analysis, setting each country as a unit of analysis. For instance, the weight that should be applied to each student should be adjusted. PISA also provides instruction on how to adjust the weight though there is still controversy over it. I will post this topic later on.\nReferences Jerrim, J., Lopez-Agudo, L. A., Marcenaro-Gutierrez, O. D., \u0026amp; Shure, N. (2017, June). To weight or not to weight?: the case of PISA data. In Proceedings of the XXVI Meeting of the Economics of Education Association, Murcia, Spain (pp. 29-30).\nOECD (2009). PISA 2006 Technical Report. OECD Publishing.\n  ","date":1602288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602354148,"objectID":"e36cfaf8bdcf36f360887a54e8384b72","permalink":"/post/why-pisa-is-difficult-to-analyze/why-is-the-pisa-data-hard-to-analyze/","publishdate":"2020-10-10T00:00:00Z","relpermalink":"/post/why-pisa-is-difficult-to-analyze/why-is-the-pisa-data-hard-to-analyze/","section":"post","summary":"By Byoung-gyu Gong\n  1. What is ILSA and PISA International large-scale assessment (ILSA) is conducted regularly by international organizations to measure and compare many different countries’ educational performance and status.","tags":["PISA","R","intsvy"],"title":"Why is PISA Difficult to Analyze","type":"post"},{"authors":[],"categories":["R","Network Analysis"],"content":"  By Byoung-gyu Gong\n Pandemic and School Reopening Issue With an outbreak of the COVID-19 pandemic, there is increasing attention to the school closer issue. Many countries implemented a strict lock-down to slow down the spread of the virus, but this came at the expense of losing our children’s learning opportunity. Despite that, the reopening school can be very costly as children can transmit the virus to their families and communities. Now that there is significant uncertainty on the impact of school reopening, we see that many countries are still under discussion on this issue. Disappointingly, we do not know much about how the school affects the children’s virus infection.\n  Purpose of the Analysis Given this situation, in this post, I will analyze children’s physical contact network at school to see how much contact happens among children. The contact network indicators at school can be a basic knowledge we can reorganize our school space, time, and activity to adjust to the post-pandemic world.\n Data Source For the network analysis, I prepared two data sets from the study of Gemnetto, Barrat, \u0026amp; Cattuto (2014) titled Mitigation of infectious disease at school: Targeted class closure vs school closure, which studied the infection network at the primary school. You can also find the data of this study from Sociopatterns. It is the open-source network data platform readily available for any studies and researches. The students’ contact network data provides a fundamental sense to design and implement a plan for contact tracing and social distance at each school level.\nThe authors collected this contact network data using a wearable device that records contact whenever students get closer over a certain threshold. The data was collected from an elementary school in France.\n Analysis For the analysis, I will use igraph package in R program. Using this package, I will calculate centrality scores to detect the central node in the school network and network structure indicators, such as network density and homophily score. Also, I will visualize the network to show you the birds-eye view of the school network.\nData pre-processing First, install the required packages and open the library.\ninstall.packages(c(\u0026quot;igraph\u0026quot;,\u0026quot;readr\u0026quot;,\u0026quot;tidyr\u0026quot;,\u0026quot;RColorBrewer\u0026quot;)) Then, you can download the data file directly from my github repository through the following code. (If there is an error message, you should try it multiple times until you can get access to it.)\n#1. Read data from the Github repository csv files urlfile1=\u0026quot;https://raw.githubusercontent.com/Arizonagong/vCIES2020_Network-Analysis/master/igraph/primaryschool.csv\u0026quot; urlfile2=\u0026quot;https://raw.githubusercontent.com/Arizonagong/vCIES2020_Network-Analysis/master/igraph/metadata_primaryschool.csv\u0026quot; D\u0026lt;-read_csv(url(urlfile1)) ## ## ── Column specification ──────────────────────────────────────────── ## cols( ## Source = col_double(), ## Target = col_double() ## ) D_meta\u0026lt;-read_csv(url(urlfile2)) ## ## ── Column specification ──────────────────────────────────────────── ## cols( ## ID = col_double(), ## Class = col_character(), ## Gender = col_character() ## ) Now, let’s look into the data set. The first dataset names as “D” is an edge list indicating contact between the students. It is represented like 1234-4424, which means one contact between the student 1234 and 4424.\nhead(D, 5) ## # A tibble: 5 x 2 ## Source Target ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1558 1567 ## 2 1560 1570 ## 3 1567 1574 ## 4 1632 1818 ## 5 1632 1866 The data set “D_meta” includes node information with students ID, class, and gender information.\nhead(D_meta, 5) ## # A tibble: 5 x 3 ## ID Class Gender ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1426 5B M ## 2 1427 5B F ## 3 1428 5B M ## 4 1429 5B F ## 5 1430 5B M Then, I just created “frequency” column in the edgelist, “D” to give an edge attribute for each edges with their contact frequency, and deleted all edges having 0 contact.\n#2. Manage dataset B\u0026lt;-as.data.frame(table(D)) # Create an edge weight column named \u0026quot;Freq\u0026quot; B1\u0026lt;-subset(B,Freq\u0026gt;0) # Delete all the edges having weight equal to 0 head(B1, 5) ## Source Target Freq ## 1 1426 1427 27 ## 240 1426 1428 45 ## 241 1427 1428 4 ## 479 1426 1429 75 ## 480 1427 1429 100 Then, we can now create an igraph object. igraph object is different from the data frame and requires totally different grammar from the code handling general data frame. igraph is specially designed to handle large-sized complex network data with high speed. The igraph object, “Stucont” includes edgelist, nodelist, and their attributes.\n#3. Create an igraph object from the dataframes Stucont\u0026lt;-graph_from_data_frame(B1, directed = FALSE, vertices = D_meta) E(Stucont)$weight\u0026lt;-E(Stucont)$Freq # Assigning edge attribute to each edge Stucont ## IGRAPH 31f002e UNW- 242 8317 -- ## + attr: name (v/c), Class (v/c), Gender (v/c), Freq (e/n), weight (e/n) ## + edges from 31f002e (vertex names): ## [1] 1426--1427 1426--1428 1427--1428 1426--1429 1427--1429 1428--1429 ## [7] 1426--1430 1427--1430 1428--1430 1429--1430 1426--1431 1427--1431 ## [13] 1428--1431 1429--1431 1430--1431 1426--1434 1427--1434 1428--1434 ## [19] 1429--1434 1430--1434 1431--1434 1426--1435 1427--1435 1428--1435 ## [25] 1429--1435 1430--1435 1431--1435 1434--1435 1426--1437 1427--1437 ## [31] 1428--1437 1429--1437 1430--1437 1431--1437 1434--1437 1435--1437 ## [37] 1426--1439 1427--1439 1428--1439 1429--1439 1430--1439 1431--1439 ## [43] 1434--1439 1435--1439 1437--1439 1426--1441 1427--1441 1428--1441 ## + ... omitted several edges  Exploring basic network features gsize shows us the number of edges, and gorder shows the number of nodes\ngsize(Stucont) ## [1] 8317 gorder(Stucont) ## [1] 242 V means the vertex (node), so with this function you can see the nodelist.\n#2. Nodelist V(Stucont) ## + 242/242 vertices, named, from 31f002e: ## [1] 1426 1427 1428 1429 1430 1431 1434 1435 1437 1439 1441 1443 1451 1452 1453 ## [16] 1457 1458 1459 1461 1465 1468 1471 1475 1477 1479 1480 1482 1483 1486 1489 ## [31] 1493 1495 1498 1500 1501 1502 1503 1504 1511 1516 1519 1520 1521 1522 1524 ## [46] 1525 1528 1532 1533 1538 1539 1545 1546 1548 1549 1551 1552 1555 1558 1560 ## [61] 1562 1563 1564 1567 1570 1572 1574 1578 1579 1580 1585 1592 1594 1601 1603 ## [76] 1604 1606 1609 1613 1616 1617 1618 1625 1628 1630 1632 1637 1641 1643 1647 ## [91] 1648 1649 1650 1653 1656 1661 1663 1664 1665 1666 1668 1670 1673 1674 1675 ## [106] 1680 1681 1682 1684 1685 1687 1688 1695 1696 1697 1698 1700 1702 1704 1705 ## [121] 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1718 1719 1720 1722 1723 ## [136] 1727 1730 1731 1732 1735 1737 1738 1739 1741 1743 1744 1745 1746 1748 1749 ## + ... omitted several vertices E means the edge.\n#3. Edgelist E(Stucont) ## + 8317/8317 edges from 31f002e (vertex names): ## [1] 1426--1427 1426--1428 1427--1428 1426--1429 1427--1429 1428--1429 ## [7] 1426--1430 1427--1430 1428--1430 1429--1430 1426--1431 1427--1431 ## [13] 1428--1431 1429--1431 1430--1431 1426--1434 1427--1434 1428--1434 ## [19] 1429--1434 1430--1434 1431--1434 1426--1435 1427--1435 1428--1435 ## [25] 1429--1435 1430--1435 1431--1435 1434--1435 1426--1437 1427--1437 ## [31] 1428--1437 1429--1437 1430--1437 1431--1437 1434--1437 1435--1437 ## [37] 1426--1439 1427--1439 1428--1439 1429--1439 1430--1439 1431--1439 ## [43] 1434--1439 1435--1439 1437--1439 1426--1441 1427--1441 1428--1441 ## [49] 1429--1441 1431--1441 1434--1441 1435--1441 1437--1441 1439--1441 ## [55] 1426--1443 1427--1443 1428--1443 1429--1443 1430--1443 1431--1443 ## + ... omitted several edges Each node have attributes such as ID, class, and gender. There are missing values so we will change “unknown” into NA.\n#4. Attributes V(Stucont)$Gender ## [1] \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; ## [8] \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; ## [15] \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; ## [22] \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; ## [29] \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; ## [36] \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; ## [43] \u0026quot;Unknown\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; ## [50] \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; ## [57] \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; ## [64] \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; ## [71] \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; ## [78] \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;Unknown\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; ## [85] \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;Unknown\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; ## [92] \u0026quot;M\u0026quot; \u0026quot;Unknown\u0026quot; \u0026quot;Unknown\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; ## [99] \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;Unknown\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; ## [106] \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; ## [113] \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; ## [120] \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;Unknown\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; ## [127] \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; ## [134] \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;Unknown\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; ## [141] \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;Unknown\u0026quot; ## [148] \u0026quot;Unknown\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;Unknown\u0026quot; ## [155] \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; ## [162] \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;Unknown\u0026quot; ## [169] \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; ## [176] \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;Unknown\u0026quot; ## [183] \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; ## [190] \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;Unknown\u0026quot; ## [197] \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; ## [204] \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;Unknown\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; ## [211] \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; ## [218] \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; ## [225] \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; ## [232] \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; ## [239] \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;F\u0026quot; \u0026quot;F\u0026quot; V(Stucont)$Gender[V(Stucont)$Gender==\u0026#39;Unknown\u0026#39;] \u0026lt;- NA The adjacency matrix is the most important indication of the network. However, igraph does not store the network data in the adjacency matrix format. Still, you can represent the network in the matrix format as below. We can know that the contact network is a symmetric and undirected network from the adjacency matrix, which means there is no arrow on edges.\n#5. Adjacency matrix Stucont[c(1:10),c(1:10)] ## 10 x 10 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot; ## [[ suppressing 10 column names \u0026#39;1426\u0026#39;, \u0026#39;1427\u0026#39;, \u0026#39;1428\u0026#39; ... ]] ## ## 1426 . 27 45 75 19 43 8 12 23 27 ## 1427 27 . 4 100 4 63 20 5 44 13 ## 1428 45 4 . 9 4 16 2 4 19 14 ## 1429 75 100 9 . 9 75 11 5 62 36 ## 1430 19 4 4 9 . 15 4 7 4 3 ## 1431 43 63 16 75 15 . 43 16 42 41 ## 1434 8 20 2 11 4 43 . 3 8 8 ## 1435 12 5 4 5 7 16 3 . 6 11 ## 1437 23 44 19 62 4 42 8 6 . 29 ## 1439 27 13 14 36 3 41 8 11 29 .  Measuring centrality Centrality is the most important indicator of the network. The centrality measure shows which node has the highest contact frequency in the network. To understand more about the centrality measure, please visit my lecture page on the Youtube channel. I will not explain the details of each centrality measure in this post.\nI identified the nodes recording the highest score in each of the centrality measures and found that student 1551 shows the highest centrality in the degree and betweenness centrality measure.\n#1. Degree centrality Stucont_deg\u0026lt;-degree(Stucont,mode=c(\u0026quot;All\u0026quot;)) V(Stucont)$degree\u0026lt;-Stucont_deg which.max(Stucont_deg) ## 1551 ## 56 #2. Eigenvector centrality Stucont_eig \u0026lt;- evcent(Stucont)$vector V(Stucont)$Eigen\u0026lt;-Stucont_eig which.max(Stucont_eig) ## 1665 ## 99 #3. Betweenness centrality Stucont_bw\u0026lt;-betweenness(Stucont, directed = FALSE) V(Stucont)$betweenness\u0026lt;-Stucont_bw which.max(Stucont_bw) ## 1551 ## 56  Measuring network structure Each network has its unique structural features.\nFirst, network density indicates how much densely the nodes are connected in the network. Also, if you’d like to know more about the network theory, please watch my lecture on Youtube.\nHere I compared the network density between the school and the class level. It shows that the school level density is 0.28 while the class level density is 0.98. Thus, there is a considerable density gap between the school and the class.\n#1. Network Density edge_density(Stucont) # Global density ## [1] 0.2852097 A1\u0026lt;-induced_subgraph(Stucont, V(Stucont)[Class==\u0026quot;1A\u0026quot;], impl=c(\u0026quot;auto\u0026quot;)) # Subgraphing into each class edge_density(A1) # Class level density ## [1] 0.9841897 We can also calculate the assortativity score, which means mingling together with the nodes having a similar attribute. For instance, we can expect that the same class students or having the same gender may have more frequent contact. The assortativity score of the class is 0.23.\n#2. Assortativity values \u0026lt;- as.numeric(factor(V(Stucont)$Class)) assortativity_nominal(Stucont, types=values) ## [1] 0.2337739 But, we do not know how big enough or small to assess the level of assortativity. We can then create a random network, which has the same probability of having an edge between every node and comparing it. The histogram indicates that the school network data is such an abnormal case having a high assortativity score according to the random network’s probability distribution.\n#2.1. Calculate the observed assortativity observed.assortativity \u0026lt;- assortativity_nominal(Stucont, types=values) results \u0026lt;- vector(\u0026#39;list\u0026#39;, 1000) for(i in 1:1000){results[[i]] \u0026lt;- assortativity_nominal(Stucont, sample(values))} #2.2. Plot the distribution of assortativity values and add a red vertical line at the original observed value hist(unlist(results), xlim = c(0,0.4)) abline(v = observed.assortativity,col = \u0026quot;red\u0026quot;, lty = 3, lwd=2)   Network Visualization The final step is the network visualization. The beauty of network analysis is that we can visually confirm the features we indicated by the numbers above. Also, the visual mapping of the network is instrumental in communicating with the audiences with data. Here, I set the size of each node with a value of degree centrality. Each different color indicates different classes.\n#1. Plotting a network with the degree centrality set.seed(1001) library(RColorBrewer) # This is the color library pal\u0026lt;-brewer.pal(length(unique(V(Stucont)$Class)), \u0026quot;Set3\u0026quot;) # Vertex color assigned per each class number plot(Stucont,edge.color = \u0026#39;black\u0026#39;,vertex.label.cex =0.5, vertex.color=pal[as.numeric(as.factor(vertex_attr(Stucont, \u0026quot;Class\u0026quot;)))], vertex.size = sqrt(Stucont_deg)/2, edge.width=sqrt(E(Stucont)$weight/800), layout = layout.fruchterman.reingold)  Community Detection Based on the networking pattern of the node, we can cluster them into several groups. We can intuitively think that the nodes will be clustered based on their affiliation with the classes. However, the result is counter-intuitive. We have ten classes in the school dataset, but the number of detected communities (cluster) is 6, which means that this school is composed of 6 different sub-network groups.\n#1. Louvain clustering lc \u0026lt;- cluster_louvain(Stucont) # Create a cluster based on the Louvain method communities(lc) # You can check which vertices belongs to which clusters. ## $`1` ## [1] \u0026quot;1426\u0026quot; \u0026quot;1427\u0026quot; \u0026quot;1428\u0026quot; \u0026quot;1429\u0026quot; \u0026quot;1430\u0026quot; \u0026quot;1431\u0026quot; \u0026quot;1434\u0026quot; \u0026quot;1435\u0026quot; \u0026quot;1437\u0026quot; \u0026quot;1439\u0026quot; ## [11] \u0026quot;1441\u0026quot; \u0026quot;1443\u0026quot; \u0026quot;1451\u0026quot; \u0026quot;1452\u0026quot; \u0026quot;1453\u0026quot; \u0026quot;1457\u0026quot; \u0026quot;1458\u0026quot; \u0026quot;1459\u0026quot; \u0026quot;1461\u0026quot; \u0026quot;1465\u0026quot; ## [21] \u0026quot;1468\u0026quot; \u0026quot;1471\u0026quot; \u0026quot;1475\u0026quot; \u0026quot;1477\u0026quot; \u0026quot;1479\u0026quot; \u0026quot;1480\u0026quot; \u0026quot;1482\u0026quot; \u0026quot;1483\u0026quot; \u0026quot;1486\u0026quot; \u0026quot;1489\u0026quot; ## [31] \u0026quot;1493\u0026quot; \u0026quot;1495\u0026quot; \u0026quot;1498\u0026quot; \u0026quot;1501\u0026quot; \u0026quot;1502\u0026quot; \u0026quot;1511\u0026quot; \u0026quot;1516\u0026quot; \u0026quot;1520\u0026quot; \u0026quot;1522\u0026quot; \u0026quot;1563\u0026quot; ## [41] \u0026quot;1578\u0026quot; \u0026quot;1585\u0026quot; \u0026quot;1592\u0026quot; \u0026quot;1637\u0026quot; \u0026quot;1668\u0026quot; \u0026quot;1750\u0026quot; \u0026quot;1751\u0026quot; \u0026quot;1824\u0026quot; \u0026quot;1885\u0026quot; ## ## $`2` ## [1] \u0026quot;1656\u0026quot; \u0026quot;1661\u0026quot; \u0026quot;1663\u0026quot; \u0026quot;1664\u0026quot; \u0026quot;1665\u0026quot; \u0026quot;1666\u0026quot; \u0026quot;1670\u0026quot; \u0026quot;1673\u0026quot; \u0026quot;1674\u0026quot; \u0026quot;1675\u0026quot; ## [11] \u0026quot;1680\u0026quot; \u0026quot;1681\u0026quot; \u0026quot;1682\u0026quot; \u0026quot;1684\u0026quot; \u0026quot;1687\u0026quot; \u0026quot;1688\u0026quot; \u0026quot;1695\u0026quot; \u0026quot;1696\u0026quot; \u0026quot;1697\u0026quot; \u0026quot;1698\u0026quot; ## [21] \u0026quot;1745\u0026quot; \u0026quot;1765\u0026quot; \u0026quot;1779\u0026quot; \u0026quot;1908\u0026quot; \u0026quot;1912\u0026quot; \u0026quot;1920\u0026quot; ## ## $`3` ## [1] \u0026quot;1711\u0026quot; \u0026quot;1752\u0026quot; \u0026quot;1753\u0026quot; \u0026quot;1757\u0026quot; \u0026quot;1759\u0026quot; \u0026quot;1760\u0026quot; \u0026quot;1761\u0026quot; \u0026quot;1764\u0026quot; \u0026quot;1766\u0026quot; \u0026quot;1767\u0026quot; ## [11] \u0026quot;1768\u0026quot; \u0026quot;1770\u0026quot; \u0026quot;1772\u0026quot; \u0026quot;1774\u0026quot; \u0026quot;1775\u0026quot; \u0026quot;1778\u0026quot; \u0026quot;1783\u0026quot; \u0026quot;1787\u0026quot; \u0026quot;1789\u0026quot; \u0026quot;1790\u0026quot; ## [21] \u0026quot;1792\u0026quot; \u0026quot;1796\u0026quot; \u0026quot;1798\u0026quot; \u0026quot;1799\u0026quot; ## ## $`4` ## [1] \u0026quot;1500\u0026quot; \u0026quot;1503\u0026quot; \u0026quot;1504\u0026quot; \u0026quot;1519\u0026quot; \u0026quot;1521\u0026quot; \u0026quot;1524\u0026quot; \u0026quot;1525\u0026quot; \u0026quot;1528\u0026quot; \u0026quot;1532\u0026quot; \u0026quot;1533\u0026quot; ## [11] \u0026quot;1538\u0026quot; \u0026quot;1539\u0026quot; \u0026quot;1545\u0026quot; \u0026quot;1546\u0026quot; \u0026quot;1548\u0026quot; \u0026quot;1549\u0026quot; \u0026quot;1601\u0026quot; \u0026quot;1618\u0026quot; \u0026quot;1630\u0026quot; \u0026quot;1632\u0026quot; ## [21] \u0026quot;1653\u0026quot; \u0026quot;1705\u0026quot; \u0026quot;1730\u0026quot; \u0026quot;1797\u0026quot; \u0026quot;1802\u0026quot; \u0026quot;1803\u0026quot; \u0026quot;1805\u0026quot; \u0026quot;1807\u0026quot; \u0026quot;1815\u0026quot; \u0026quot;1818\u0026quot; ## [31] \u0026quot;1819\u0026quot; \u0026quot;1821\u0026quot; \u0026quot;1831\u0026quot; \u0026quot;1835\u0026quot; \u0026quot;1836\u0026quot; \u0026quot;1837\u0026quot; \u0026quot;1847\u0026quot; \u0026quot;1857\u0026quot; \u0026quot;1865\u0026quot; \u0026quot;1866\u0026quot; ## [41] \u0026quot;1880\u0026quot; \u0026quot;1888\u0026quot; \u0026quot;1892\u0026quot; \u0026quot;1895\u0026quot; \u0026quot;1910\u0026quot; ## ## $`5` ## [1] \u0026quot;1603\u0026quot; \u0026quot;1604\u0026quot; \u0026quot;1606\u0026quot; \u0026quot;1609\u0026quot; \u0026quot;1613\u0026quot; \u0026quot;1616\u0026quot; \u0026quot;1617\u0026quot; \u0026quot;1625\u0026quot; \u0026quot;1628\u0026quot; \u0026quot;1641\u0026quot; ## [11] \u0026quot;1643\u0026quot; \u0026quot;1647\u0026quot; \u0026quot;1648\u0026quot; \u0026quot;1649\u0026quot; \u0026quot;1650\u0026quot; \u0026quot;1702\u0026quot; \u0026quot;1704\u0026quot; \u0026quot;1706\u0026quot; \u0026quot;1708\u0026quot; \u0026quot;1710\u0026quot; ## [21] \u0026quot;1713\u0026quot; \u0026quot;1715\u0026quot; \u0026quot;1718\u0026quot; \u0026quot;1732\u0026quot; \u0026quot;1739\u0026quot; \u0026quot;1743\u0026quot; \u0026quot;1749\u0026quot; \u0026quot;1851\u0026quot; \u0026quot;1852\u0026quot; \u0026quot;1854\u0026quot; ## [31] \u0026quot;1855\u0026quot; \u0026quot;1858\u0026quot; \u0026quot;1861\u0026quot; \u0026quot;1863\u0026quot; \u0026quot;1872\u0026quot; \u0026quot;1877\u0026quot; \u0026quot;1883\u0026quot; \u0026quot;1887\u0026quot; \u0026quot;1889\u0026quot; \u0026quot;1890\u0026quot; ## [41] \u0026quot;1897\u0026quot; \u0026quot;1898\u0026quot; \u0026quot;1902\u0026quot; \u0026quot;1906\u0026quot; \u0026quot;1907\u0026quot; \u0026quot;1911\u0026quot; \u0026quot;1913\u0026quot; \u0026quot;1916\u0026quot; \u0026quot;1917\u0026quot; \u0026quot;1919\u0026quot; ## [51] \u0026quot;1922\u0026quot; ## ## $`6` ## [1] \u0026quot;1551\u0026quot; \u0026quot;1552\u0026quot; \u0026quot;1555\u0026quot; \u0026quot;1558\u0026quot; \u0026quot;1560\u0026quot; \u0026quot;1562\u0026quot; \u0026quot;1564\u0026quot; \u0026quot;1567\u0026quot; \u0026quot;1570\u0026quot; \u0026quot;1572\u0026quot; ## [11] \u0026quot;1574\u0026quot; \u0026quot;1579\u0026quot; \u0026quot;1580\u0026quot; \u0026quot;1594\u0026quot; \u0026quot;1685\u0026quot; \u0026quot;1700\u0026quot; \u0026quot;1707\u0026quot; \u0026quot;1709\u0026quot; \u0026quot;1712\u0026quot; \u0026quot;1714\u0026quot; ## [21] \u0026quot;1719\u0026quot; \u0026quot;1720\u0026quot; \u0026quot;1722\u0026quot; \u0026quot;1723\u0026quot; \u0026quot;1727\u0026quot; \u0026quot;1731\u0026quot; \u0026quot;1735\u0026quot; \u0026quot;1737\u0026quot; \u0026quot;1738\u0026quot; \u0026quot;1741\u0026quot; ## [31] \u0026quot;1744\u0026quot; \u0026quot;1746\u0026quot; \u0026quot;1748\u0026quot; \u0026quot;1763\u0026quot; \u0026quot;1780\u0026quot; \u0026quot;1782\u0026quot; \u0026quot;1795\u0026quot; \u0026quot;1800\u0026quot; \u0026quot;1801\u0026quot; \u0026quot;1809\u0026quot; ## [41] \u0026quot;1820\u0026quot; \u0026quot;1822\u0026quot; \u0026quot;1833\u0026quot; \u0026quot;1838\u0026quot; \u0026quot;1843\u0026quot; \u0026quot;1859\u0026quot; \u0026quot;1909\u0026quot; #2. Plotting the Betweenness Centrality network with the community detection set.seed(1001) # To duplicate the computer process and create exactly the same network repetitively you should set the seed. plot(lc, Stucont, edge.color = \u0026#39;black\u0026#39;,vertex.label.cex =0.5, vertex.color=pal[as.numeric(as.factor(vertex_attr(Stucont, \u0026quot;Class\u0026quot;)))], vertex.size = sqrt(Stucont_bw)/3, edge.width=sqrt(E(Stucont)$weight/800), layout = layout.fruchterman.reingold) So far, we walked through the whole process of network analysis and visualization. Although this analysis is descriptive, we could learn a lot about the school’s student dynamics only with this small information. Also, it provided information on the latent community structure that was not visible before the analysis.\nThe students’ physical contact data set is rare because it has a privacy issue to measure the contact information. Thus, this analysis provides us such a precious insight into the students’ physical network in school.\n  ","date":1601769600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601848559,"objectID":"eb4630fde087f0c0f51c3594412e7012","permalink":"/post/aspb/network/","publishdate":"2020-10-04T00:00:00Z","relpermalink":"/post/aspb/network/","section":"post","summary":"By Byoung-gyu Gong\n Pandemic and School Reopening Issue With an outbreak of the COVID-19 pandemic, there is increasing attention to the school closer issue. Many countries implemented a strict lock-down to slow down the spread of the virus, but this came at the expense of losing our children’s learning opportunity.","tags":["Network analysis","Network density","Assortativity","Degree centrality","Eivenvector centrality","Betweenness centrality","Community detection"],"title":"Student contact network at school: Spread of the virus","type":"post"},{"authors":[],"categories":["R","Network analysis"],"content":" By Byoung-gyu Gong\nTheory Albert-Barabasi model is well-known network generation method, adding new vertices at each time steps. The connections added to the existing vertices at each time are proportional to the degree of the given vertice. The generated network shows the rich-get-richer phenomenon in the networked world as new connection tends to increase proportional to the previous degree.\nThe master equation is as follows: \\[P[i] = k[i]^a + b\\]\nwhere k[i] is the in-degree of vertex i in the current time, a is an exponent to create a power-law distribution, and b is attractiveness of the vertices with no degree.\n R script Albert-Barabasi network creation We can create the Albert-Barabasi network data set using “sample_pa” function in igraph package. 100 means the number of vertices to create, power=3 is the ‘a’ in the above formula, and m is the number of edges to add for each time step.\nA\u0026lt;-sample_pa(100, power=3, m=2, directed=FALSE, algorithm = \u0026quot;psumtree\u0026quot;) A ## IGRAPH 73ee564 U--- 100 197 -- Barabasi graph ## + attr: name (g/c), power (g/n), m (g/n), zero.appeal (g/n), algorithm ## | (g/c) ## + edges from 73ee564: ## [1] 1-- 2 1-- 3 2-- 3 1-- 4 2-- 4 2-- 5 1-- 5 2-- 6 1-- 6 2-- 7 1-- 7 2-- 8 ## [13] 1-- 8 2-- 9 1-- 9 1--10 2--10 2--11 1--11 1--12 2--12 1--13 2--13 2--14 ## [25] 1--14 2--15 1--15 1--16 2--16 1--17 2--17 2--18 1--18 1--19 2--19 1--20 ## [37] 2--20 1--21 2--21 1--22 2--22 2--23 1--23 2--24 1--24 1--25 2--25 1--26 ## [49] 2--26 1--27 2--27 1--28 2--28 2--29 1--29 2--30 1--30 2--31 1--31 2--32 ## [61] 1--32 2--33 1--33 1--34 2--34 2--35 1--35 1--36 2--36 1--37 2--37 1--38 ## [73] 2--38 1--39 2--39 1--40 2--40 2--41 1--41 1--42 2--42 1--43 2--43 1--44 ## + ... omitted several edges The above network data object does not include any information on the time step as an edge attribute (we need information on when the given edge will be added to generate the network). So, we need to create a time step information and merge it to the network data object A.\nT is the vector of the time step. The reason why the same number repeats twice is that we modeled the network to add 2 edges per each time step.\nT\u0026lt;-rep(x=seq(2,99), each=2) T\u0026lt;-append(1,T) T ## [1] 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 10 10 11 11 12 12 13 13 ## [26] 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21 22 22 23 23 24 24 25 25 26 ## [51] 26 27 27 28 28 29 29 30 30 31 31 32 32 33 33 34 34 35 35 36 36 37 37 38 38 ## [76] 39 39 40 40 41 41 42 42 43 43 44 44 45 45 46 46 47 47 48 48 49 49 50 50 51 ## [101] 51 52 52 53 53 54 54 55 55 56 56 57 57 58 58 59 59 60 60 61 61 62 62 63 63 ## [126] 64 64 65 65 66 66 67 67 68 68 69 69 70 70 71 71 72 72 73 73 74 74 75 75 76 ## [151] 76 77 77 78 78 79 79 80 80 81 81 82 82 83 83 84 84 85 85 86 86 87 87 88 88 ## [176] 89 89 90 90 91 91 92 92 93 93 94 94 95 95 96 96 97 97 98 98 99 99 Then, we can merge the time step data into the network data. Now, we can find “time (e/n)” in the igraph object, which means that the time step information is now assigned as an edge attribute.\nE(A)$time\u0026lt;-T V(A)$name\u0026lt;-V(A) A ## IGRAPH 73ee564 UN-- 100 197 -- Barabasi graph ## + attr: name (g/c), power (g/n), m (g/n), zero.appeal (g/n), algorithm ## | (g/c), name (v/n), time (e/n) ## + edges from 73ee564 (vertex names): ## [1] 1-- 2 1-- 3 2-- 3 1-- 4 2-- 4 2-- 5 1-- 5 2-- 6 1-- 6 2-- 7 1-- 7 2-- 8 ## [13] 1-- 8 2-- 9 1-- 9 1--10 2--10 2--11 1--11 1--12 2--12 1--13 2--13 2--14 ## [25] 1--14 2--15 1--15 1--16 2--16 1--17 2--17 2--18 1--18 1--19 2--19 1--20 ## [37] 2--20 1--21 2--21 1--22 2--22 2--23 1--23 2--24 1--24 1--25 2--25 1--26 ## [49] 2--26 1--27 2--27 1--28 2--28 2--29 1--29 2--30 1--30 2--31 1--31 2--32 ## [61] 1--32 2--33 1--33 1--34 2--34 2--35 1--35 1--36 2--36 1--37 2--37 1--38 ## [73] 2--38 1--39 2--39 1--40 2--40 2--41 1--41 1--42 2--42 1--43 2--43 1--44 ## + ... omitted several edges The following script was directly copied from the blog of Dr. Esteban Moro. You should set the right directory before implementing the below chunk of codes as it creates 100 pages of image in the directory folder.\n#this version of the script has been tested on igraph 1.0.1 #load libraries require(igraph,RcolorBrewer) install.packages(\u0026quot;RColorBrewer\u0026quot;) library(RColorBrewer) #generate a cool palette for the graph (darker colors = older nodes) YlOrBr.pal \u0026lt;- colorRampPalette(brewer.pal(8,\u0026quot;YlOrRd\u0026quot;)) #colors for the nodes are chosen from the very beginning V(A)$color \u0026lt;- rev(YlOrBr.pal(vcount(A)))[as.numeric(V(A)$name)] #time in the edges goes from 1 to 300. We kick off at time 3 ti \u0026lt;- 2 #remove edges which are not present gt \u0026lt;- delete_edges(A,which(E(A)$time \u0026gt; ti)) # Generate first layout using graphopt with normalized coordinates. # This places the initially connected set of nodes in the middle. # If you use fruchterman.reingold it will place that initial set in the outer ring. layout.old \u0026lt;- norm_coords(layout.graphopt(gt), xmin = -1, xmax = 1, ymin = -1, ymax = 1) #total time of the dynamics total_time \u0026lt;- max(E(A)$time) #This is the time interval for the animation. In this case is taken to be 1/10 #of the time (i.e. 10 snapshots) between adding two consecutive nodes dt \u0026lt;- 0.1 #Output for each frame will be a png with HD size 1600x900 :) png(file=\u0026quot;example%03d.png\u0026quot;, width=1600,height=900) #Time loop starts for(time in seq(3,total_time,dt)){ #remove edges which are not present gt \u0026lt;- delete_edges(A,which(E(A)$time \u0026gt; time)) #with the new graph, we update the layout a little bit layout.new \u0026lt;- layout_with_fr(gt,coords=layout.old,niter=10,start.temp=0.05,grid=\u0026quot;nogrid\u0026quot;) #plot the new graph plot(gt,layout=layout.new, vertex.label=\u0026quot;\u0026quot;,vertex.size=1+2*log(degree(gt)), vertex.frame.color=V(A)$color,edge.width=1.5, asp=9/16,margin=-0.15) #use the new layout in the next round layout.old \u0026lt;- layout.new } dev.off() Once you created the png image files of the network for each time step, then you can create an animation using “ffmpeg” application. If you already installed ‘brew’ in your computer, then you can easily install the “ffmpeg” with very short code like this in your terminal:\n$ brew install ffmpeg Then, you can create an network animation with the following code in your terminal:\nffmpeg -r 10 -i example%03d.png -b:v 20M output.mp4    ","date":1601683200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601788891,"objectID":"58269560f3d456061047bf987bc6c14f","permalink":"/post/temporal-network-simulation-albert-barabasi-model/","publishdate":"2020-10-03T00:00:00Z","relpermalink":"/post/temporal-network-simulation-albert-barabasi-model/","section":"post","summary":"By Byoung-gyu Gong\nTheory Albert-Barabasi model is well-known network generation method, adding new vertices at each time steps. The connections added to the existing vertices at each time are proportional to the degree of the given vertice.","tags":["Network analysis","Network simulation","Temporal network","Albert-Barabasi model"],"title":"Temporal network simulation: Albert-Barabasi model","type":"post"},{"authors":["Byoung-gyu Gong"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Byoung-gyu Gong","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Byoung-gyu Gong","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]
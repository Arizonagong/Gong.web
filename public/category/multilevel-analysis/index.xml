<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Multilevel analysis | Byoung-gyu Gong</title>
    <link>/category/multilevel-analysis/</link>
      <atom:link href="/category/multilevel-analysis/index.xml" rel="self" type="application/rss+xml" />
    <description>Multilevel analysis</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 10 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Multilevel analysis</title>
      <link>/category/multilevel-analysis/</link>
    </image>
    
    <item>
      <title>Analyzing Nested Data: Comparing Multiple Approaches</title>
      <link>/post/nested-data/analyzing-nested-data-comparing-multiple-approaches/</link>
      <pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/nested-data/analyzing-nested-data-comparing-multiple-approaches/</guid>
      <description>


&lt;div id=&#34;what-is-nested-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;1. What is Nested Data&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Not all data is created to be independent. We often find that the real world data does not meet the strict statistical presumption that each observation should be independent and identically distributed, so-called iid. In many cases of social science studies, the observations sampled from a population tend to correlate because they belong to the same group, region, and culture. Sample data having such common and correlative trait is called nested data.&lt;/p&gt;
&lt;p&gt;For instance, students in the same school share some traits derived from the school and district features. Students in the same school or district tend to have similar socio-economic status (SES) because students are assigned to the schools based on their residential location. Students in the same school also tend to share the same teachers, school leadership, and school resource. In this account, student samples in the same schools may violate iid principle correlating with each other.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;2. Example Data Analysis&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;It can be costly to ignore the nested data structure. This post will show you a difference between the models considering and not considering the nested data structure.&lt;/p&gt;
&lt;p&gt;I used the international students’ data file for this analysis. The students studied in a foreign country through a one-year study abroad program to raise intercultural understanding. I analyzed to what extent the self-efficacy level predicts or explains the level of students’ intercultural understanding.&lt;/p&gt;
&lt;div id=&#34;explorative-data-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;2.1. Explorative Data Analysis&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;This is the summary of the data set. It tells us there are four variables. Intercultural_Understanding and Self-Efficacy are the continuous variables while the rest of Major and Year are categorical variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Summary of the data
summary(Studyabroad)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Intercultural_Understanding Self_Efficacy      Major       Year   
##  Min.   :1.000               Min.   :3.000   Eng   :20   18-19:31  
##  1st Qu.:3.250               1st Qu.:4.208   Noneng:52   19-20:41  
##  Median :4.333               Median :5.190                         
##  Mean   :4.089               Mean   :4.980                         
##  3rd Qu.:5.000               3rd Qu.:5.762                         
##  Max.   :6.000               Max.   :6.000                         
##  NA&amp;#39;s   :2                   NA&amp;#39;s   :1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get a more detailed sense of the data, let’s check the distribution of the data with visualization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Scatter plot
SA &amp;lt;- ggscatter(Studyabroad, x = &amp;quot;Self_Efficacy&amp;quot;, y = &amp;quot;Intercultural_Understanding&amp;quot;,color = &amp;quot;#00AFBB&amp;quot;, size = 1, alpha = 0.6, add = &amp;quot;reg.line&amp;quot;, add.params = list(color = &amp;quot;blue&amp;quot;, fill = &amp;quot;lightgray&amp;quot;),
   conf.int = TRUE, 
   cor.coef = TRUE ,rug = TRUE)+
border()                                         
# Marginal density plot of x (top panel) and y (right panel)
xplot &amp;lt;- ggdensity(Studyabroad, &amp;quot;Self_Efficacy&amp;quot;, fill = &amp;quot;lightgrey&amp;quot;)
yplot &amp;lt;- ggdensity(Studyabroad, &amp;quot;Intercultural_Understanding&amp;quot;, fill = &amp;quot;lightgrey&amp;quot;)+
rotate()
# Cleaning the plots
SA &amp;lt;- SA + rremove(&amp;quot;legend&amp;quot;)
yplot &amp;lt;- yplot + clean_theme() + rremove(&amp;quot;legend&amp;quot;) 
xplot &amp;lt;- xplot + clean_theme() + rremove(&amp;quot;legend&amp;quot;)

library(cowplot)
plot_grid(xplot, NULL, SA, yplot, ncol = 2, align = &amp;quot;hv&amp;quot;, 
      rel_widths = c(2, 1), rel_heights = c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Nested-data/2020-12-10-analyzing-nested-data-comparing-multiple-approaches.en_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simple-linear-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;2.2. Simple Linear Regression&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The easiest way to analyze this data is simple linear regression. The analysis summary indicates that the students’ self-efficacy is positively associated with the level of intercultural understanding. The coefficient estimate is 0.3914, and the p-value is significant at the 0.05 level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Lm0&amp;lt;-lm(Intercultural_Understanding ~ Self_Efficacy, data =  Studyabroad)
summary(Lm0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Intercultural_Understanding ~ Self_Efficacy, data = Studyabroad)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.9404 -0.9175  0.2527  0.9850  1.8409 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept)     2.1298     0.9314   2.287   0.0254 *
## Self_Efficacy   0.3914     0.1841   2.126   0.0372 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.253 on 67 degrees of freedom
##   (3 observations deleted due to missingness)
## Multiple R-squared:  0.06319,    Adjusted R-squared:  0.04921 
## F-statistic:  4.52 on 1 and 67 DF,  p-value: 0.0372&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;alternative-analytic-methods-for-the-nested-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;2.3. Alternative Analytic Methods for the Nested Data&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;However, the data is stratified with other grouping variables. The data ‘Studyabroad’ has a ‘Major’ variable, indicating that each student belongs to a specific major program. The students’ intercultural understanding explained by self-efficacy may be influenced by their majors.&lt;/p&gt;
&lt;p&gt;We can plot the regression line in the above scatter plot, but I will draw two different lines for each group this time. The scatter plot below shows that students in English and non-English majors have very different patterns of association. In general, the students’ intercultural understanding in English majors is related to their self-efficacy level, while the non-English major students showed no relationship between them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SA&amp;lt;-ggscatter(Studyabroad, x = &amp;quot;Self_Efficacy&amp;quot;, y = &amp;quot;Intercultural_Understanding&amp;quot;, size = 0.3,
          combine = TRUE, color = &amp;quot;Major&amp;quot;, palette = &amp;quot;jco&amp;quot;,
          add = &amp;quot;reg.line&amp;quot;, conf.int = TRUE) +
  stat_cor(aes(color = Major), method = &amp;quot;spearman&amp;quot;)
# Marginal density plot of x (top panel) and y (right panel)
xplot &amp;lt;- ggdensity(Studyabroad, &amp;quot;Self_Efficacy&amp;quot;, fill = &amp;quot;Major&amp;quot;,
                   palette = &amp;quot;jco&amp;quot;)
yplot &amp;lt;- ggdensity(Studyabroad, &amp;quot;Intercultural_Understanding&amp;quot;, fill = &amp;quot;Major&amp;quot;,palette = &amp;quot;jco&amp;quot;)+
rotate()
# Cleaning the plots
SA &amp;lt;- SA + rremove(&amp;quot;legend&amp;quot;)
yplot &amp;lt;- yplot + clean_theme() + rremove(&amp;quot;legend&amp;quot;) 
xplot &amp;lt;- xplot + clean_theme() + rremove(&amp;quot;legend&amp;quot;)

library(cowplot)
plot_grid(xplot, NULL, SA, yplot, ncol = 2, align = &amp;quot;hv&amp;quot;, 
      rel_widths = c(2, 1), rel_heights = c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Nested-data/2020-12-10-analyzing-nested-data-comparing-multiple-approaches.en_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The biggest problem of not considering the nested structure of the data is underestimating each parameter estimation’s standard error. This underestimation is fed into a more frequent rejection of the null hypothesis for the coefficient estimation. We have already seen that the simple linear regression model rejected the null hypothesis above.&lt;/p&gt;
&lt;div id=&#34;solution1.-complex-linear-regression-model-with-grouping-dummy-variables&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;2.3.1. Solution1. Complex Linear Regression Model with Grouping Dummy Variables&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;One of the immediate solutions to handle nested data issue is adding grouping dummy variables in the regression modeling.&lt;/p&gt;
&lt;p&gt;The new regression model below included ‘Major,’ one of the grouping variables, in the modeling. Comparing with the previous simple linear regression, the standard error of Self-efficacy’s coefficient increased from 0.1841 to 0.2565. Moreover, the significant test of the coefficient estimation did not reject the null hypothesis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Lm1&amp;lt;-lm(Intercultural_Understanding ~ Self_Efficacy + Major, data =  Studyabroad)
summary(Lm1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Intercultural_Understanding ~ Self_Efficacy + Major, 
##     data = Studyabroad)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.93139 -0.89425  0.05834  1.09501  1.70774 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept)     2.7438     1.0816   2.537   0.0136 *
## Self_Efficacy   0.1925     0.2565   0.750   0.4557  
## MajorNoneng     0.5228     0.4706   1.111   0.2706  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.251 on 66 degrees of freedom
##   (3 observations deleted due to missingness)
## Multiple R-squared:  0.08039,    Adjusted R-squared:  0.05253 
## F-statistic: 2.885 on 2 and 66 DF,  p-value: 0.06293&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;solution2.-hierarchical-linear-modeling-hlm-or-multilevel-modeling&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;2.3.2. Solution2. Hierarchical Linear Modeling (HLM) or Multilevel Modeling&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Then, at this time, I will use hierarchical linear modeling analysis with the lme4 package to compare it with the previous simple linear regression model.&lt;/p&gt;
&lt;p&gt;First, I will create a null model to calculate the intraclass correlation (ICC). This number indicates the proportion of the outcome variable’s total variation explained by between-group variation. The ICC calculation for this data is about 12%, meaning that the students’ difference in affiliation to majors explains the 12% out of the total variation of the intercultural understanding.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
Lme0&amp;lt;-lmer(Intercultural_Understanding ~ 1+(1|Major), data=Studyabroad)
summary(Lme0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: Intercultural_Understanding ~ 1 + (1 | Major)
##    Data: Studyabroad
## 
## REML criterion at convergence: 231.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.2609 -0.6583  0.1094  0.8609  1.4064 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  Major    (Intercept) 0.2082   0.4563  
##  Residual             1.5430   1.2422  
## Number of obs: 70, groups:  Major, 2
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)   3.9655     0.3606   10.99&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ICC Calculation
library(sjstats)
icc(Lme0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # Intraclass Correlation Coefficient
## 
##      Adjusted ICC: 0.119
##   Conditional ICC: 0.119&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, I added self-efficacy as a predictor. The summary statistics indicate that the self-efficacy coefficient is not significant due to its increased standard error estimation. This tells us that similar to the regression model with the grouping dummy variables, the multilevel modeling also avoids underestimating the standard error lowering probability to reject the null hypothesis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lmerTest)
Lme1&amp;lt;-lmer(Intercultural_Understanding ~ 1 + Self_Efficacy + (1+Self_Efficacy|Major), data=Studyabroad, na.action = na.exclude)
summary(Lme1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML. t-tests use Satterthwaite&amp;#39;s method [
## lmerModLmerTest]
## Formula: 
## Intercultural_Understanding ~ 1 + Self_Efficacy + (1 + Self_Efficacy |  
##     Major)
##    Data: Studyabroad
## 
## REML criterion at convergence: 223.9
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.4245 -0.5919  0.1211  0.8689  1.4253 
## 
## Random effects:
##  Groups   Name          Variance Std.Dev. Corr 
##  Major    (Intercept)   34.365   5.862         
##           Self_Efficacy  1.717   1.310    -1.00
##  Residual                1.422   1.193         
## Number of obs: 69, groups:  Major, 2
## 
## Fixed effects:
##               Estimate Std. Error     df t value Pr(&amp;gt;|t|)
## (Intercept)     0.2977     4.2955 0.6712   0.069    0.960
## Self_Efficacy   0.8874     0.9615 0.6535   0.923    0.582
## 
## Correlation of Fixed Effects:
##             (Intr)
## Self_Effccy -0.999
## convergence code: 0
## boundary (singular) fit: see ?isSingular&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;3. Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This post addressed analyzing nested data, which often violates iid assumption for the inferential statistics. I compared the simple regression model with a more complex regression model, including the group dummy variable and multilevel model. This comparison showed that the simple regression model without considering nested data structure tends to underestimate the coefficient’s standard error, thus more frequently rejecting the null hypothesis in the significant test. Thus, to avoid this underestimation of standard error, it is generally recommended to use a more complex regression model with group dummy variables or multilevel modeling analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why is PISA Difficult to Analyze</title>
      <link>/post/why-pisa-is-difficult-to-analyze/why-is-the-pisa-data-hard-to-analyze/</link>
      <pubDate>Sat, 10 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/why-pisa-is-difficult-to-analyze/why-is-the-pisa-data-hard-to-analyze/</guid>
      <description>


&lt;p&gt;By Byoung-gyu Gong&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/Why-PISA-is-difficult-to-analyze/featured.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div id=&#34;what-is-ilsa-and-pisa&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;1. What is ILSA and PISA&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;International large-scale assessment (ILSA) is conducted regularly by international organizations to measure and compare many different countries’ educational performance and status. The Programme for International Student Assessment (PISA) by OECD is the most well-known ILSA testing member and non-member partner country’s 15-year-old students every two years. It measures students’ reading, math, and science using a computer-based instrument. It also surveys to collect more detailed background information of each student, teacher, and school. The PISA 2006 collected this massive size of data from 398,750 students. Thus, it can be said that the PISA is the most comprehensive large-sized international student assessment data set available now.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-is-it-difficult-to-analyze-pisa-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;2. Why is it difficult to analyze PISA data&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Despite the insightful knowledge we can gain from this large international students’ data set, PISA is not easily accessible to most of the researchers to analyze it. That is not because the data access is limited but because it is complexly structured, requesting sophisticated data pre-processing. Even Jerrim, Lopez-Agudo, Marcenaro-Gutierrez, and Shure (2017) said, “in spite of their acquired relevance, there are few studies which really account for the complex survey and test designs that they present and follow the technical procedures suggested by their developers” (p.1). I will illustrate what caution is needed to analyze the data set and provides R solution in this post using a data set of PISA 2006. &lt;/p&gt;
&lt;div id=&#34;two-staged-sampling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;2.1. Two-staged sampling&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The sampling design of the PISA is the reason why the analysis procedure is so complicated. The PISA does not use random sampling and instead follows two-staged sampling: sample schools and then sample students in the participating schools. It makes the sampling errors of population estimates increase. It conflicts with most computer software designed to assume random sampling for statistical analysis and requests more sophisticated analysis procedures. Once the assumption of random sampling is violated, the student data can depend on each other because they may share common school characteristics.&lt;br /&gt;
 &lt;/p&gt;
&lt;p&gt;To avoid such bias, PISA made some complementary measures.
 &lt;/p&gt;
&lt;p&gt;Once you look at the data table imported from the PISA 2006, you would see some strange variable names such as PV1-5 for each reading, math, and science domains. Also, there are some variables having names like W_FSTR1-80. If you just naively wanted to calculate students’ mean or correlation coefficient with a fixed point score, you would be embarrassed to find these unknown variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Use the following packages for the analysis
library(intsvy)
library(dplyr)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(pisa2006,1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   X CNT SCHOOLID STIDSTD  PV1MATH  PV2MATH  PV3MATH  PV4MATH  PV5MATH  PV1READ
## 1 1 ARG        1       1 305.1799 305.9589 264.6752 320.7587 301.2852 357.0519
##    PV2READ  PV3READ  PV4READ  PV5READ  PV1SCIE  PV2SCIE  PV3SCIE  PV4SCIE
## 1 312.1348 281.6554 316.9474 316.1453 395.3131 411.1652 445.6667 351.4869
##    PV5SCIE  PV1INTR  PV2INTR  PV3INTR  PV4INTR  PV5INTR  PV1SUPP  PV2SUPP
## 1 356.1492 539.9939 480.1177 497.9912 370.1956 506.0343 339.0228 395.3084
##    PV3SUPP  PV4SUPP  PV5SUPP   PV1EPS   PV2EPS   PV3EPS   PV4EPS   PV5EPS
## 1 412.5387 313.7517 322.9412 393.4482 384.1234 354.2843 430.7471 417.6925
##     PV1ISI   PV2ISI   PV3ISI   PV4ISI   PV5ISI   PV1USE  PV2USE   PV3USE
## 1 334.7023 304.8632 323.5126 399.9755 386.9209 401.8404 383.191 385.0559
##    PV4USE  PV5USE W_FSTUWT W_FSTR1  W_FSTR2 W_FSTR3  W_FSTR4 W_FSTR5  W_FSTR6
## 1 436.342 469.911  98.8278 52.1041 139.9112 52.1041 139.9112 52.1041 139.9112
##   W_FSTR7 W_FSTR8 W_FSTR9 W_FSTR10 W_FSTR11 W_FSTR12 W_FSTR13 W_FSTR14 W_FSTR15
## 1 52.1041 52.1041 52.1041  52.1041 139.9112 139.9112  52.1041 139.9112 139.9112
##   W_FSTR16 W_FSTR17 W_FSTR18 W_FSTR19 W_FSTR20 W_FSTR21 W_FSTR22 W_FSTR23
## 1  52.1041  52.1041 139.9112 139.9112 139.9112  52.1041 139.9112  52.1041
##   W_FSTR24 W_FSTR25 W_FSTR26 W_FSTR27 W_FSTR28 W_FSTR29 W_FSTR30 W_FSTR31
## 1 139.9112  52.1041 139.9112  52.1041  52.1041  52.1041  52.1041 139.9112
##   W_FSTR32 W_FSTR33 W_FSTR34 W_FSTR35 W_FSTR36 W_FSTR37 W_FSTR38 W_FSTR39
## 1 139.9112  52.1041 139.9112 139.9112  52.1041  52.1041 139.9112 139.9112
##   W_FSTR40 W_FSTR41 W_FSTR42 W_FSTR43 W_FSTR44 W_FSTR45 W_FSTR46 W_FSTR47
## 1 139.9112  52.1041 139.9112  52.1041 139.9112  52.1041 139.9112  52.1041
##   W_FSTR48 W_FSTR49 W_FSTR50 W_FSTR51 W_FSTR52 W_FSTR53 W_FSTR54 W_FSTR55
## 1  52.1041  52.1041  52.1041 139.9112 139.9112  52.1041 139.9112 139.9112
##   W_FSTR56 W_FSTR57 W_FSTR58 W_FSTR59 W_FSTR60 W_FSTR61 W_FSTR62 W_FSTR63
## 1  52.1041  52.1041 139.9112 139.9112 139.9112  52.1041 139.9112  52.1041
##   W_FSTR64 W_FSTR65 W_FSTR66 W_FSTR67 W_FSTR68 W_FSTR69 W_FSTR70 W_FSTR71
## 1 139.9112  52.1041 139.9112  52.1041  52.1041  52.1041  52.1041 139.9112
##   W_FSTR72 W_FSTR73 W_FSTR74 W_FSTR75 W_FSTR76 W_FSTR77 W_FSTR78 W_FSTR79
## 1 139.9112  52.1041 139.9112 139.9112  52.1041  52.1041 139.9112 139.9112
##   W_FSTR80    ESCS
## 1 139.9112 -2.0048&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, what are these variables in the PISA table?
 &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sampling-weight&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;2.2. Sampling Weight&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;To make each sampled student from each school represent each country’s entire population, we need to give a weight to each student variables. The random sample secures each sampled student’s equal probability, but two-stage sampling needs to adjust the probability with a complicated calculation of the weight. Weight is an inverse of probability to be selected as a sample. For instance, we selected ten students out of 100 students, then the probability to be selected is 0.1, and the weight is 10.
 &lt;/p&gt;
&lt;p&gt;In PISA, each school’s probability to be selected is proportional to its size, which is called PPS sampling method. The schools with larger sizes have a higher probability of selection, but the larger schools have a proportionally less within-school probability of selection. The sum of students’ weight for each country is the total number of the students’ population assumed for the study.
 &lt;/p&gt;
&lt;p&gt;There are two options to apply students’ weight for the PISA analysis, applying final weight(W_FSTUWT), and replicated weight(F_FSTR1-80), but in this post, I do not tackle the replicated weight because it is more related to the estimation of standard error, but not have an impact to the mean, correlation, and regression coefficent estimate.
 &lt;/p&gt;
&lt;p&gt;Let me show you an example here with R code. W_FSTUWT is the final students’ weight. You can see that students in the same school have approximately the same school weights with a little variation. School number 00001 has weights around 98, and 00002 has it at about 89.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Select Argentina students&amp;#39; weight information.
WEIGHT&amp;lt;-pisa2006 %&amp;gt;% select(CNT, SCHOOLID, W_FSTUWT) %&amp;gt;% filter(CNT==&amp;quot;ARG&amp;quot;)
# Create a dataframe that can compare Argentina students in three different schools
A&amp;lt;-WEIGHT %&amp;gt;% filter(SCHOOLID==&amp;quot;1&amp;quot;) %&amp;gt;% slice(1:3)
B&amp;lt;-WEIGHT %&amp;gt;% filter(SCHOOLID==&amp;quot;2&amp;quot;) %&amp;gt;% slice(1:3)
C&amp;lt;-WEIGHT %&amp;gt;% filter(SCHOOLID==&amp;quot;3&amp;quot;) %&amp;gt;% slice(1:3)
rbind(A,B,C)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   CNT SCHOOLID W_FSTUWT
## 1 ARG        1  98.8278
## 2 ARG        1  91.7075
## 3 ARG        1  98.8278
## 4 ARG        2  89.3573
## 5 ARG        2  89.6970
## 6 ARG        2  89.6970
## 7 ARG        3 112.7806
## 8 ARG        3 117.0837
## 9 ARG        3 117.0837&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; 
The summation of the students’ weight is equal to the population total population of 15-years-old students in Argentina. It is 523,047.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;WEIGHT %&amp;gt;% filter(CNT==&amp;quot;ARG&amp;quot;) %&amp;gt;% group_by(CNT) %&amp;gt;% summarize(sum=sum(W_FSTUWT))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   CNT       sum
##   &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 ARG   523048.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Also, we can compare the difference in variable means between raw scores and weighted scores. Comparing with the graph ‘B’ with the graph ‘A,’ we can recognize that the graph ‘B’ is more slightly deviated from the regression line, although the difference is negligible for most countries. Some of the countries in specific variables, the bias to the estimate can be significantly large so we should apply the final student weight for the analysis all the time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create weighted mean score (W_MEAN) and non-weighted means score of PV1SCIE (science score)
# Calculate national mean
PV1SCIE&amp;lt;-pisa2006 %&amp;gt;% select(CNT, PV1SCIE, W_FSTUWT) %&amp;gt;% group_by(CNT) %&amp;gt;%  mutate(W_MEAN=weighted.mean(PV1SCIE,W_FSTUWT, na.rm=TRUE), MEAN=mean(PV1SCIE, na.rm=TRUE)) %&amp;gt;% ungroup() %&amp;gt;% select(CNT, W_MEAN, MEAN) %&amp;gt;% unique()

# Create a scatter plot to compare the discrepancy between 
library(ggplot2)
library(ggpubr)
# Create a scatter plot with raw score means (unweighted mean)
Non_weight&amp;lt;-ggplot(PV1SCIE, aes(x=MEAN, y=MEAN)) + geom_point() + geom_text(data=PV1SCIE, aes(label=CNT), position=position_jitter(width=0.1,height=0.1), size=3) + geom_smooth(method=lm, se=TRUE)+stat_cor(method = &amp;quot;pearson&amp;quot;)
# Create a scatter plot with unweighted mean and weighted mean
Weight_non_weight&amp;lt;-ggplot(PV1SCIE, aes(x=MEAN, y=W_MEAN)) + geom_point() + geom_text(data=PV1SCIE, aes(label=CNT), position=position_jitter(width=0.1,height=0.1), size=3) + geom_smooth(method=lm, se=TRUE)+stat_cor(method = &amp;quot;pearson&amp;quot;)
# Compore these two
ggarrange(Non_weight, Weight_non_weight, 
          labels = c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;),
          ncol = 2, nrow = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;
## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Why-PISA-is-difficult-to-analyze/2020-10-10-why-is-the-pisa-data-hard-to-analyze.en_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plausible-valuepv&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;2.3. Plausible Value(PV)&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;PV is called plausible value, and each domain has five PVs. For instance, math has PV1MATH - PV5MATH, and science has PV1SCIE - PV5SCIE. The PVs are introduced here to adjust measurement error that can be caused by
- the concept to be measured is not clear,
- students physical and psychological condition affected at the moment of testing, and
- the testing environment on a day of testing.
Thus, if we have five different plausible values distributed in a certain range of raw scores, we can avoid point estimate of the students’ competency, which has a high risk of measurement error as mentioned above. It means that we interpret students’ competency as a probability distribution among the plausible values, not as a fixed point. In addition to this rational, the PV was adopted in a practical and efficient standpoint. In PISA tests, students do not take all the test items as it is time-consuming, and each student takes different sets of tests, while the test items missing in each student are treated as a missing value. It creates five different sets of literally plausible score values of the students.
 &lt;/p&gt;
&lt;p&gt;I created two columns of mean data to compare weighted PV means and weighted means of each country. However, as we can see from the below table, there is no significant difference between the weighted PV mean and just the weighted mean with the final weight. The OECD also admitted that PV value does not bring significant impact to reduce bias in the estimate (OECD, 2009)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a dataframe with weighted PV means (weighted by final weight)
PVSCIE&amp;lt;-pisa.mean.pv(pvlabel=&amp;quot;SCIE&amp;quot;, by=&amp;quot;CNT&amp;quot;, data=pisa2006)
# Create a dataframe with weighted means (weighted by final weight)
SCIE&amp;lt;-pisa2006 %&amp;gt;% group_by(CNT) %&amp;gt;% summarise(Mean=(weighted.mean(PV4SCIE,W_FSTUWT, na.rm=TRUE))) %&amp;gt;% mutate(Mean=round(Mean, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Merge&amp;lt;-left_join(PVSCIE, SCIE, by=&amp;quot;CNT&amp;quot;) %&amp;gt;% select(CNT, Freq, Mean.x, Mean.y) %&amp;gt;% rename(PVmean=&amp;quot;Mean.x&amp;quot;, Rawmean=&amp;quot;Mean.y&amp;quot;)
head(Merge,15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    CNT  Freq PVmean Rawmean
## 1  ARG  4339 391.24  391.33
## 2  AUS 14170 526.88  526.90
## 3  AUT  4927 510.84  510.95
## 4  AZE  5184 382.33  381.78
## 5  BEL  8857 510.36  510.62
## 6  BGR  4498 434.08  433.27
## 7  BRA  9295 390.33  389.43
## 8  CAN 22646 534.47  534.28
## 9  CHE 12192 511.52  511.71
## 10 CHL  5233 438.18  438.26
## 11 COL  4478 388.04  388.60
## 12 CZE  5932 512.86  512.69
## 13 DEU  4891 515.65  516.32
## 14 DNK  4532 495.89  495.05
## 15 ESP 19604 488.42  487.51&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;3. Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;As we examined so far, the students’ final weight is a significant factor to reduce bias in estimation. Although PV value has less impact to the estimation and OECD also admit it, but still it should be best to follow the PISA instruction.&lt;/p&gt;
&lt;p&gt;In this post, I only explored the estimates at each country level. However, it is very different for the case of cross-national analysis, setting each country as a unit of analysis. For instance, the weight that should be applied to each student should be adjusted. PISA also provides instruction on how to adjust the weight though there is still controversy over it. I will post this topic later on.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Jerrim, J., Lopez-Agudo, L. A., Marcenaro-Gutierrez, O. D., &amp;amp; Shure, N. (2017, June). To weight or not to weight?: the case of PISA data. In Proceedings of the XXVI Meeting of the Economics of Education Association, Murcia, Spain (pp. 29-30).&lt;/p&gt;
&lt;p&gt;OECD (2009). PISA 2006 Technical Report. OECD Publishing.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
